{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e1a4ea",
   "metadata": {},
   "source": [
    "!pip install git+https://github.com/openai/whisper.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b34d27",
   "metadata": {},
   "source": [
    "!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1398ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "948ee0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WATCH LIVE | Fed Chair Powell Discusses Latest Fed Rate Hike'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_video_url = \"https://www.youtube.com/watch?v=NT2H9iyd-ms\"\n",
    "youtube_video = YouTube(youtube_video_url)\n",
    "youtube_video.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a36031e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_age_restricted',\n",
       " '_author',\n",
       " '_embed_html',\n",
       " '_fmt_streams',\n",
       " '_initial_data',\n",
       " '_js',\n",
       " '_js_url',\n",
       " '_metadata',\n",
       " '_player_config_args',\n",
       " '_publish_date',\n",
       " '_title',\n",
       " '_vid_info',\n",
       " '_watch_html',\n",
       " 'age_restricted',\n",
       " 'allow_oauth_cache',\n",
       " 'author',\n",
       " 'bypass_age_gate',\n",
       " 'caption_tracks',\n",
       " 'captions',\n",
       " 'channel_id',\n",
       " 'channel_url',\n",
       " 'check_availability',\n",
       " 'description',\n",
       " 'embed_html',\n",
       " 'embed_url',\n",
       " 'fmt_streams',\n",
       " 'from_id',\n",
       " 'initial_data',\n",
       " 'js',\n",
       " 'js_url',\n",
       " 'keywords',\n",
       " 'length',\n",
       " 'metadata',\n",
       " 'publish_date',\n",
       " 'rating',\n",
       " 'register_on_complete_callback',\n",
       " 'register_on_progress_callback',\n",
       " 'stream_monostate',\n",
       " 'streaming_data',\n",
       " 'streams',\n",
       " 'thumbnail_url',\n",
       " 'title',\n",
       " 'use_oauth',\n",
       " 'vid_info',\n",
       " 'video_id',\n",
       " 'views',\n",
       " 'watch_html',\n",
       " 'watch_url']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(youtube_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e3a7e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Stream: itag=\"17\" mime_type=\"video/3gpp\" res=\"144p\" fps=\"7fps\" vcodec=\"mp4v.20.3\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">\n",
      "<Stream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">\n",
      "<Stream: itag=\"22\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.64001F\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">\n",
      "<Stream: itag=\"137\" mime_type=\"video/mp4\" res=\"1080p\" fps=\"30fps\" vcodec=\"avc1.640028\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"248\" mime_type=\"video/webm\" res=\"1080p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"136\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.64001f\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"247\" mime_type=\"video/webm\" res=\"720p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"135\" mime_type=\"video/mp4\" res=\"480p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"244\" mime_type=\"video/webm\" res=\"480p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"134\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"243\" mime_type=\"video/webm\" res=\"360p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"133\" mime_type=\"video/mp4\" res=\"240p\" fps=\"30fps\" vcodec=\"avc1.4d4015\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"242\" mime_type=\"video/webm\" res=\"240p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"160\" mime_type=\"video/mp4\" res=\"144p\" fps=\"30fps\" vcodec=\"avc1.4d400c\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"278\" mime_type=\"video/webm\" res=\"144p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n",
      "<Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\">\n",
      "<Stream: itag=\"140\" mime_type=\"audio/mp4\" abr=\"128kbps\" acodec=\"mp4a.40.2\" progressive=\"False\" type=\"audio\">\n",
      "<Stream: itag=\"249\" mime_type=\"audio/webm\" abr=\"50kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">\n",
      "<Stream: itag=\"250\" mime_type=\"audio/webm\" abr=\"70kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">\n",
      "<Stream: itag=\"251\" mime_type=\"audio/webm\" abr=\"160kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">\n"
     ]
    }
   ],
   "source": [
    "# have a list of video and audio streams of varying quality. \n",
    "# We can iterate over this list and observe the different resolutions and framerates.\n",
    "for stream in youtube_video.streams:\n",
    "  print(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdf6d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stream in youtube_video.streams.filter(only_audio=True):\n",
    "  print(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9b351e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\">"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_video.streams.filter(only_audio=True).order_by(\"abr\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd5603be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hwuac\\\\Documents\\\\dev\\\\OpenAI\\\\test.mp4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_video.streams.filter(only_audio=True).order_by(\"abr\").asc().first().download(filename='test.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257ad69",
   "metadata": {},
   "source": [
    "To ignore any additional sound in the beginning of vedio. we use ffmpeg to trimm the audio file at the 375 second mark where he starts with good afternoon, continue for 2715 seconds, and chop off the rest of the audio. The result will be saved in a new file called fed_meeting_trimmed.mp4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -ss 378 -i fed_meeting.mp4 -t 1000 fed_meeting_trim2.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874bbbd",
   "metadata": {},
   "source": [
    "https://medium.com/the-research-nest/how-to-setup-openais-whisper-model-on-windows-10-11-df001d5a350b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4756d",
   "metadata": {},
   "source": [
    "pip install -U openai-whisper\n",
    "pip install ffmpeg-python\n",
    "conda install -c conda-forge ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611de1e4",
   "metadata": {},
   "source": [
    "!whisper \"fed_meeting_trim1.mp4\" --model base.en --fp16 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29bdbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "815c24e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at 2023-06-10 19:22:19.319205\n",
      "ended at 2023-06-10 19:54:51.969256\n",
      "time elapsed: 0:32:32.650051\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# save a timestamp before transcription\n",
    "t1 = datetime.datetime.now()\n",
    "print(f\"started at {t1}\")\n",
    "\n",
    "# do the transcription\n",
    "result = model.transcribe(\"fed_meeting_trim1.mp4\", fp16=False)\n",
    "\n",
    "# show time elapsed after transcription is complete.\n",
    "t2 = datetime.datetime.now()\n",
    "print(f\"ended at {t2}\")\n",
    "print(f\"time elapsed: {t2 - t1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143609ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__ior__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eaf14ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Good afternoon. My colleagues and I are strongly committed to bringing inflation back down to our 2% goal. We have both the tools that we need and the resolve it will take to restore price stability on behalf of American families and businesses. Price stability is the responsibility of the Federal Reserve and serves as the bedrock of our economy. Without price stability, the economy does not work for anyone. In particular, without price stability, we will not achieve a sustained period of strong labor market conditions that benefit all. Today, the FOMC raised our policy interest rate by 75 basis points and we continue to anticipate that ongoing increases will be appropriate. We are moving our policy stance purposefully to a level that will be sufficiently restrictive to return inflation to 2%. In addition, we are continuing the process of significantly reducing the size of our balance sheet. Restoring price stability will likely require maintaining a restrictive stance of policy for some time. I will have more to say about today's monetary policy actions after briefly reviewing economic developments. The U.S. economy has slowed significantly from last year's rapid pace. Real GDP rose at a pace of 2.6% last quarter but is unchanged so far this year. Recent indicators point to modest growth of spending and production this quarter. Growth in consumer spending has slowed from last year's rapid pace in part reflecting lower real disposable income and tighter financial conditions. Activity in the housing sector has weakened significantly, largely reflecting higher mortgage rates. Higher interest rates and slower output growth also appear to be weighing on business fixed investment. Despite the slowdown in growth, the labor market remains extremely tight with the unemployment rate at a 50-year low. Job vacancies still very high and wage growth elevated. Job gains have been robust with employment rising by an average of 289,000 jobs per month over August and September. Although job vacancies have moved below their highs and the pace of job gains has slowed from earlier in the year, the labor market continues to be out of balance, with demand substantially exceeding the supply of available workers. The labor force participation rate is little changed since the beginning of the year. Inflation remains well above our longer run goal of 2%. Over the 12 months ending in September, total PCE prices rose at 6.2%, excluding the volatile food and energy categories, core PCE prices rose at 5.1%. The recent inflation data again have come in higher than expected. Price pressures remained evident across a broad range of goods and services. Russia's war against Ukraine has boosted prices for energy and food and has created additional upward pressure on inflation. Despite elevated inflation, longer term inflation expectations appear to remain well anchored, as reflected in a broad range of surveys of households, businesses and forecasters, as well as measures from financial markets. But that is not grounds for complacency. The longer the current amount of high inflation continues, the greater the chance that expectations of higher inflation will become entrenched. The Fed's monetary policy actions are guided by our mandate to promote maximum employment and stable prices for the American people. My colleagues and I are acutely aware that high inflation imposes significant hardship and is at erodes purchasing power, especially for those least able to meet the higher costs of essentials like food, housing and transportation. We are highly attentive to the risks that high inflation poses to both sides of our mandate and we're strongly committed to returning inflation to our 2% objective. At today's meeting, the committee raised the target range for the federal funds rate by 75 basis points. And we are continuing the process of significantly reducing the size of our balance sheet, which plays an important role in affirming the stance of monetary policy. With today's action, we've raised interest rates by 3 and 3-quarters percentage points this year. We anticipate that ongoing increases in the target range for the federal funds rate will be appropriate in order to attain a stance of monetary policy that is sufficiently restrictive to return inflation to 2% over time. Financial conditions have tightened significantly in response to our policy actions and we are seeing the effects on demand in the most interest rate sensitive sectors of the economy such as housing. It will take time, however, for the full effects of monetary restraint to be realized, especially on inflation. That's why we say in our statement that in determining the pace of future increases in the target range, we will take into account the cumulative tightening of monetary policy and the lags with which monetary policy affects economic activity and inflation. At some point, as I've said in the last two press conferences, it will become appropriate to slow the pace of increases as we approach the level of interest rates that will be sufficiently restrictive to bring inflation down to our 2% goal. There is significant uncertainty around that level of interest rates. Even so, we still have some ways to go. And incoming data since our last meeting suggests that the ultimate level of interest rates will be higher than previously expected. Our decisions will depend on the totality of incoming data and their implications for the outlook for economic activity and inflation. We will continue to make our decisions meeting by being and communicate our thinking as clearly as possible. We're taking forceful steps to moderate demand so that it comes into better alignment with supply. Our overarching focus is using our tools to bring inflation back down to our 2% goal and to keep longer term inflation expectations well anchored. Reducing inflation is likely to require a sustained period of below trend growth and some softening of labor market conditions. Restoring price stability is essential to set the stage for achieving maximum employment and stable prices in the longer run. The historical record caution strongly against prematurely loosening policy. We will stay the course until the job is done. To conclude, we understand that our actions affect communities, families and businesses across the country. Everything we do is in service to our public mission. We at the Fed will do everything we can to achieve our maximum employment and price stability goals. Thank you and I look forward to your questions. Thank you, Colby. Thank you. Colby Smith with the Financial Times. On the need to slow the pace of rate increases at some point. Is a is a downshift contingent on a string of better inflation data specifically between now and let's say the December meeting? Is that something that the Fed could potentially proceed with independent of that data given the lag the fact that you mentioned? So a couple of things on that. We do need to see inflation coming down decisively and good at evidence of that would be a series of down monthly readings. Of course that's what we'd all love to see. But that's I've never thought of that as the appropriate test for slowing the pace of increases or for identifying the appropriately restrictive level that we're aiming for. We need to bring our policy stance down to a level that's sufficiently restrictive to bring inflation down to our 2% objective over the medium term. How will we know that we've reached that level? Well, we'll take into account the full range of analysis and data that bear on that question guided by our assessment of how much financial conditions have tightened the effects of that tightening is actually having on the real economy and on inflation. Taking into consideration lags as I mentioned. We will be looking at real rates, for example, all across the yield curve and all other financial conditions. And as we make that assessment. Hi, how are you not with with rotors? Look, I'm sure there's going to be tons of confusion out there about whether this means you're going to slow in December or not. Would you say that the bias right now is not for another 75 basis point increase? What I want to do is put that question of pace in the context of our broader tightening program, if I may, and to talk about the statement language along the way. So I think you can think about our our tightening program has really addressing three questions. The first of which was and has been how fast to go. The second is how high to raise our policy rate and the third will be eventually how long to remain at a restrictive level. On the first question, how fast to tighten policy. It's been very important that we move expeditiously and we have clearly done so. We've moved three and three quarters percent since March, admittedly from a base of zero. It's a historically fast pace and that's that's certainly appropriate given the persistence and strength and inflation and the low level from which we started. So now we come to the second question, which is how high to raise our policy rate and we're saying that we'd raise that rate to a level that's sufficiently restrictive to bring inflation to our two percent target over time. And we put that into our into our post meeting statement because that really does become the important question we think now is how far to go and I'll talk more about that. We think there's some ground to cover but before we meet that test and that's why we say that ongoing rate increases will be appropriate. And as I mentioned incoming data between the meetings, both the strong labor market report, but particularly the CPI report do suggest to me that we may ultimately move to higher levels than we thought at the time of the September meeting. That level is very uncertain though and I would say, you know, we're going to find it over time. Of course, with the lags between policy and economic activity, there's a lot of uncertainty. So we note that in determining the pace of future increases will take into account the cumulative tightening of monetary policy as well as the lags with monetary policy affects economic activity and inflation. So I would say as we come closer to that level, move more into restrictive territory. So that's the question of speed becomes less important than the second and third questions. And that's why I've said it the last two press conferences that at some point it will become appropriate to slow the pace of increases. So that time is coming and it may come as soon as the next meeting or the one after that. No decision has been made. It is likely we'll have a discussion about this at the next meeting, a discussion. To be clear, let me say again, the question of when to moderate the pace of increases is now much less important than the question of how high to raise rates and how long to keep monetary policy restricted, which really will be our principal focus. So I think I could follow up on that. To what degree was there an importance or weight given to a need to signal this possibility now given all the concerns really around the globe about fed policy sort of driving ahead and everybody else dealing with their own stress as a result. I think I'm pleased that we have moved as fast as we have. I don't think we've over tightened. I think there's very difficult to make a case that our current level is too tight given that inflation still runs well above the federal funds rate. So I think that at this meeting, the last two meetings, as I've mentioned, I've said that we that there would come a point and this was a meeting at which we had a discussion about what that might mean. And we did discuss this and as I mentioned, we'll discuss it again in December. But there's no, I don't have any sense that we've over tightened or moved too fast. I think it's been good in a successful program that we've gotten this far this fast. Remember that we still think there's a need for ongoing rate increases. And we have some ground left to cover here and cover it we will. Nick Temeros of the Wall Street Journal. Chair Powell, core BCE inflation on a three or six month annualized basis and on a 12 month basis has been running in the high for close to 5%. Is there any reason to think you won't have to raise rates at least above that level to be confident that you are imparting enough restraint to bring inflation down? Well, this is the question of does the policy rate need to get above the inflation rate? And I would say there are a range of views on that. That's the classic Taylor principle view. But I would think you'd look more at a forward, you know, a forward looking measure of inflation to look at that. But I think the answer is we'll want to get the policy rate to a level where it is where the real interest rate is positive. And we will want to do that. I do not think of it as the as the single and only touch down though. I think you put some weight on that. You also put some weight on rates across the curve very few people borrow at the short end of the federal funds rate, for example. So households and businesses if they're very, you know, meaningfully positive interest rates all across the curve for them. Cred spread spreads are larger. So borrowing rates are significantly higher. And I think financial conditions have tightened quite a bit. So I would look at that as an important feature. I'd put some weight on it, but I wouldn't say it's something that is the single dominant thing to look at. If I could follow up. What is your best assessment or the staff's best assessment right now of the current rate of underlying inflation? I don't have a specific number for you there. There are many, many models that look at that. And I mean, one way to look at it is that it's a pretty stationary object. And that when inflation runs above that level for sure substantially, above for some time, you'll see it move up. But the movement will be fairly gradual. So I think that's what that's what the principal models would would tend to say, but I wouldn't want to land on any one assessment. There are many different. As you know, many different people publish an assessment of underlying inflation. Thank you. Hi, Chair Powell. Thank you for taking our questions. Gina Smiley with New York Times. I wonder, do you see any evidence at this stage that inflation is or is it risk of becoming entrenched? His inflation becoming entrenched. So I guess I would start by pointing to expectations. So if we saw longer term expectations moving up, that would be very troubling. And they were moving up a little bit in the middle part of this year and they move now back down. That's one piece of data. Short term inflation expectation that moved up between the last meeting and this meeting. And we don't think those are as indicative, but they may be important in the wage setting process. There's a school of thought that believes that. So that's very concerning. I guess the other thing I would say is that the longer we have, we're now 18 months into this episode of high inflation. And we don't have a clearly identified scientific way of understanding at what point inflation becomes entrenched. And so the thing we need to do from a risk management standpoint is to use our tools forcefully but thoughtfully and get inflation under control, get it down to 2% get it behind us. That's what we really need to do and what we're strongly committed to doing. Rachel. Thank you. Hi, Terpowell. Thank you for taking your questions. Rachel Segal from the Washington Post. The statement points to the lag times. I'm wondering if you can walk us through how you judge those lag effects. What that timeline looks like over the coming months or even a year and where you would expect it to show up in different parts of the economy. Yeah, so let me the way I would think about that.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9464b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed894a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy = pd.read_csv(\"spy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97f3ef3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>average</th>\n",
       "      <th>barCount</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-02 14:31:15</td>\n",
       "      <td>387.13</td>\n",
       "      <td>387.17</td>\n",
       "      <td>386.75</td>\n",
       "      <td>386.83</td>\n",
       "      <td>82173.0</td>\n",
       "      <td>386.884</td>\n",
       "      <td>444</td>\n",
       "      <td>Good afternoon. My colleagues and I are stron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-02 14:31:20</td>\n",
       "      <td>386.80</td>\n",
       "      <td>387.00</td>\n",
       "      <td>386.80</td>\n",
       "      <td>386.91</td>\n",
       "      <td>38918.0</td>\n",
       "      <td>386.913</td>\n",
       "      <td>234</td>\n",
       "      <td>We have both the tools that we need and the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-02 14:31:25</td>\n",
       "      <td>386.94</td>\n",
       "      <td>386.96</td>\n",
       "      <td>386.55</td>\n",
       "      <td>386.64</td>\n",
       "      <td>48165.0</td>\n",
       "      <td>386.749</td>\n",
       "      <td>232</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-02 14:31:30</td>\n",
       "      <td>386.64</td>\n",
       "      <td>386.95</td>\n",
       "      <td>386.59</td>\n",
       "      <td>386.86</td>\n",
       "      <td>30688.0</td>\n",
       "      <td>386.760</td>\n",
       "      <td>190</td>\n",
       "      <td>Price stability is the responsibility of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-02 14:31:35</td>\n",
       "      <td>386.91</td>\n",
       "      <td>386.92</td>\n",
       "      <td>386.81</td>\n",
       "      <td>386.87</td>\n",
       "      <td>22270.0</td>\n",
       "      <td>386.862</td>\n",
       "      <td>130</td>\n",
       "      <td>Without price stability, the economy does not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>2022-11-02 15:16:05</td>\n",
       "      <td>380.30</td>\n",
       "      <td>380.48</td>\n",
       "      <td>380.30</td>\n",
       "      <td>380.39</td>\n",
       "      <td>40401.0</td>\n",
       "      <td>380.418</td>\n",
       "      <td>265</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>2022-11-02 15:16:10</td>\n",
       "      <td>380.37</td>\n",
       "      <td>380.49</td>\n",
       "      <td>380.24</td>\n",
       "      <td>380.45</td>\n",
       "      <td>40709.0</td>\n",
       "      <td>380.378</td>\n",
       "      <td>221</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>2022-11-02 15:16:15</td>\n",
       "      <td>380.45</td>\n",
       "      <td>380.48</td>\n",
       "      <td>380.38</td>\n",
       "      <td>380.39</td>\n",
       "      <td>12465.0</td>\n",
       "      <td>380.425</td>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>2022-11-02 15:16:20</td>\n",
       "      <td>380.38</td>\n",
       "      <td>380.38</td>\n",
       "      <td>380.28</td>\n",
       "      <td>380.36</td>\n",
       "      <td>24107.0</td>\n",
       "      <td>380.324</td>\n",
       "      <td>139</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>2022-11-02 15:16:25</td>\n",
       "      <td>380.35</td>\n",
       "      <td>380.36</td>\n",
       "      <td>380.28</td>\n",
       "      <td>380.34</td>\n",
       "      <td>12428.0</td>\n",
       "      <td>380.311</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date    open    high     low   close   volume  average  \\\n",
       "0    2022-11-02 14:31:15  387.13  387.17  386.75  386.83  82173.0  386.884   \n",
       "1    2022-11-02 14:31:20  386.80  387.00  386.80  386.91  38918.0  386.913   \n",
       "2    2022-11-02 14:31:25  386.94  386.96  386.55  386.64  48165.0  386.749   \n",
       "3    2022-11-02 14:31:30  386.64  386.95  386.59  386.86  30688.0  386.760   \n",
       "4    2022-11-02 14:31:35  386.91  386.92  386.81  386.87  22270.0  386.862   \n",
       "..                   ...     ...     ...     ...     ...      ...      ...   \n",
       "538  2022-11-02 15:16:05  380.30  380.48  380.30  380.39  40401.0  380.418   \n",
       "539  2022-11-02 15:16:10  380.37  380.49  380.24  380.45  40709.0  380.378   \n",
       "540  2022-11-02 15:16:15  380.45  380.48  380.38  380.39  12465.0  380.425   \n",
       "541  2022-11-02 15:16:20  380.38  380.38  380.28  380.36  24107.0  380.324   \n",
       "542  2022-11-02 15:16:25  380.35  380.36  380.28  380.34  12428.0  380.311   \n",
       "\n",
       "     barCount                                               text  \n",
       "0         444   Good afternoon. My colleagues and I are stron...  \n",
       "1         234   We have both the tools that we need and the r...  \n",
       "2         232                                                NaN  \n",
       "3         190   Price stability is the responsibility of the ...  \n",
       "4         130   Without price stability, the economy does not...  \n",
       "..        ...                                                ...  \n",
       "538       265                                                NaN  \n",
       "539       221                                                NaN  \n",
       "540        69                                                NaN  \n",
       "541       139                                                NaN  \n",
       "542        91                                                NaN  \n",
       "\n",
       "[543 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for segment in result['segments']:\n",
    "   second = int(segment['start'])\n",
    "   second = second - (second % 5)\n",
    "   spy.loc[second / 5, 'text'] = segment['text']\n",
    "\n",
    "spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a91d4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- ---------------\n",
      "alabaster                     0.7.12\n",
      "anaconda-client               1.11.2\n",
      "anaconda-navigator            2.4.0\n",
      "anaconda-project              0.11.1\n",
      "anyio                         3.5.0\n",
      "appdirs                       1.4.4\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "arrow                         1.2.3\n",
      "astroid                       2.14.2\n",
      "astropy                       5.1\n",
      "asttokens                     2.0.5\n",
      "atomicwrites                  1.4.0\n",
      "attrs                         22.1.0\n",
      "Automat                       20.2.0\n",
      "autopep8                      1.6.0\n",
      "Babel                         2.11.0\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.4\n",
      "backports.tempfile            1.0\n",
      "backports.weakref             1.0.post1\n",
      "bcrypt                        3.2.0\n",
      "beautifulsoup4                4.11.1\n",
      "binaryornot                   0.4.4\n",
      "black                         22.6.0\n",
      "bleach                        4.1.0\n",
      "bokeh                         2.4.3\n",
      "boltons                       23.0.0\n",
      "Bottleneck                    1.3.5\n",
      "brotlipy                      0.7.0\n",
      "certifi                       2023.5.7\n",
      "cffi                          1.15.1\n",
      "chardet                       4.0.0\n",
      "charset-normalizer            2.0.4\n",
      "click                         8.0.4\n",
      "cloudpickle                   2.0.0\n",
      "clyent                        1.2.2\n",
      "colorama                      0.4.6\n",
      "colorcet                      3.0.1\n",
      "comm                          0.1.2\n",
      "conda                         23.3.1\n",
      "conda-build                   3.24.0\n",
      "conda-content-trust           0.1.3\n",
      "conda-pack                    0.6.0\n",
      "conda-package-handling        2.0.2\n",
      "conda_package_streaming       0.7.0\n",
      "conda-repo-cli                1.0.41\n",
      "conda-token                   0.4.0\n",
      "conda-verify                  3.4.2\n",
      "constantly                    15.1.0\n",
      "contourpy                     1.0.5\n",
      "cookiecutter                  1.7.3\n",
      "cryptography                  39.0.1\n",
      "cssselect                     1.1.0\n",
      "cycler                        0.11.0\n",
      "cytoolz                       0.12.0\n",
      "daal4py                       2023.0.2\n",
      "dask                          2022.7.0\n",
      "datashader                    0.14.4\n",
      "datashape                     0.5.4\n",
      "debugpy                       1.5.1\n",
      "decorator                     5.1.1\n",
      "defusedxml                    0.7.1\n",
      "diff-match-patch              20200713\n",
      "dill                          0.3.6\n",
      "distributed                   2022.7.0\n",
      "docstring-to-markdown         0.11\n",
      "docutils                      0.18.1\n",
      "entrypoints                   0.4\n",
      "et-xmlfile                    1.1.0\n",
      "eventkit                      1.0.0\n",
      "executing                     0.8.3\n",
      "fastjsonschema                2.16.2\n",
      "ffmpeg                        1.4\n",
      "ffmpeg-python                 0.2.0\n",
      "filelock                      3.9.0\n",
      "flake8                        6.0.0\n",
      "Flask                         2.2.2\n",
      "flit_core                     3.6.0\n",
      "fonttools                     4.25.0\n",
      "fsspec                        2022.11.0\n",
      "future                        0.18.3\n",
      "gensim                        4.3.0\n",
      "glob2                         0.7\n",
      "greenlet                      2.0.1\n",
      "h5py                          3.7.0\n",
      "HeapDict                      1.0.1\n",
      "holoviews                     1.15.4\n",
      "huggingface-hub               0.10.1\n",
      "hvplot                        0.8.2\n",
      "hyperlink                     21.0.0\n",
      "ib-insync                     0.9.85\n",
      "idna                          3.4\n",
      "imagecodecs                   2021.8.26\n",
      "imageio                       2.26.0\n",
      "imagesize                     1.4.1\n",
      "imbalanced-learn              0.10.1\n",
      "importlib-metadata            4.11.3\n",
      "incremental                   21.3.0\n",
      "inflection                    0.5.1\n",
      "iniconfig                     1.1.1\n",
      "intake                        0.6.7\n",
      "intervaltree                  3.1.0\n",
      "ipykernel                     6.19.2\n",
      "ipython                       8.10.0\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    7.6.5\n",
      "isort                         5.9.3\n",
      "itemadapter                   0.3.0\n",
      "itemloaders                   1.0.4\n",
      "itsdangerous                  2.0.1\n",
      "jedi                          0.18.1\n",
      "jellyfish                     0.9.0\n",
      "Jinja2                        3.1.2\n",
      "jinja2-time                   0.2.0\n",
      "jmespath                      0.10.0\n",
      "joblib                        1.1.1\n",
      "json5                         0.9.6\n",
      "jsonpatch                     1.32\n",
      "jsonpointer                   2.1\n",
      "jsonschema                    4.17.3\n",
      "jupyter                       1.0.0\n",
      "jupyter_client                7.3.4\n",
      "jupyter-console               6.6.2\n",
      "jupyter_core                  5.2.0\n",
      "jupyter-server                1.23.4\n",
      "jupyterlab                    3.5.3\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab_server             2.19.0\n",
      "jupyterlab-widgets            1.0.0\n",
      "keyring                       23.4.0\n",
      "kiwisolver                    1.4.4\n",
      "lazy-object-proxy             1.6.0\n",
      "libarchive-c                  2.9\n",
      "llvmlite                      0.39.1\n",
      "locket                        1.0.0\n",
      "lxml                          4.9.1\n",
      "lz4                           3.1.3\n",
      "Markdown                      3.4.1\n",
      "MarkupSafe                    2.1.1\n",
      "matplotlib                    3.7.0\n",
      "matplotlib-inline             0.1.6\n",
      "mccabe                        0.7.0\n",
      "menuinst                      1.4.19\n",
      "mistune                       0.8.4\n",
      "mkl-fft                       1.3.1\n",
      "mkl-random                    1.2.2\n",
      "mkl-service                   2.4.0\n",
      "mock                          4.0.3\n",
      "more-itertools                9.1.0\n",
      "mplfinance                    0.12.9b7\n",
      "mpmath                        1.2.1\n",
      "msgpack                       1.0.3\n",
      "multipledispatch              0.6.0\n",
      "munkres                       1.1.4\n",
      "mypy-extensions               0.4.3\n",
      "navigator-updater             0.3.0\n",
      "nbclassic                     0.5.2\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     6.5.4\n",
      "nbformat                      5.7.0\n",
      "nest-asyncio                  1.5.6\n",
      "networkx                      2.8.4\n",
      "nltk                          3.7\n",
      "notebook                      6.5.2\n",
      "notebook_shim                 0.2.2\n",
      "numba                         0.56.4\n",
      "numexpr                       2.8.4\n",
      "numpy                         1.23.5\n",
      "numpydoc                      1.5.0\n",
      "openai-whisper                20230314\n",
      "openpyxl                      3.0.10\n",
      "packaging                     22.0\n",
      "pandas                        1.5.3\n",
      "pandocfilters                 1.5.0\n",
      "panel                         0.14.3\n",
      "param                         1.12.3\n",
      "paramiko                      2.8.1\n",
      "parsel                        1.6.0\n",
      "parso                         0.8.3\n",
      "partd                         1.2.0\n",
      "pathlib                       1.0.1\n",
      "pathspec                      0.10.3\n",
      "patsy                         0.5.3\n",
      "pep8                          1.7.1\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.4.0\n",
      "pip                           22.3.1\n",
      "pkginfo                       1.9.6\n",
      "platformdirs                  2.5.2\n",
      "plotly                        5.9.0\n",
      "pluggy                        1.0.0\n",
      "ply                           3.11\n",
      "pooch                         1.4.0\n",
      "poyo                          0.5.0\n",
      "prometheus-client             0.14.1\n",
      "prompt-toolkit                3.0.36\n",
      "Protego                       0.1.16\n",
      "psutil                        5.9.0\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "py                            1.11.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycodestyle                   2.10.0\n",
      "pycosat                       0.6.4\n",
      "pycparser                     2.21\n",
      "pyct                          0.5.0\n",
      "pycurl                        7.45.1\n",
      "PyDispatcher                  2.0.5\n",
      "pydocstyle                    6.3.0\n",
      "pyerfa                        2.0.0\n",
      "pyflakes                      3.0.1\n",
      "Pygments                      2.11.2\n",
      "PyHamcrest                    2.0.2\n",
      "PyJWT                         2.4.0\n",
      "pylint                        2.16.2\n",
      "pylint-venv                   2.3.0\n",
      "pyls-spyder                   0.4.0\n",
      "PyNaCl                        1.5.0\n",
      "pyodbc                        4.0.34\n",
      "pyOpenSSL                     23.0.0\n",
      "pyparsing                     3.0.9\n",
      "PyQt5                         5.15.7\n",
      "PyQt5-sip                     12.11.0\n",
      "PyQtWebEngine                 5.15.4\n",
      "pyrsistent                    0.18.0\n",
      "PySocks                       1.7.1\n",
      "pytest                        7.1.2\n",
      "python-dateutil               2.8.2\n",
      "python-lsp-black              1.2.1\n",
      "python-lsp-jsonrpc            1.0.0\n",
      "python-lsp-server             1.7.1\n",
      "python-slugify                5.0.2\n",
      "python-snappy                 0.6.1\n",
      "pytoolconfig                  1.2.5\n",
      "pytube                        15.0.0\n",
      "pytz                          2022.7\n",
      "pyviz-comms                   2.0.2\n",
      "PyWavelets                    1.4.1\n",
      "pywin32                       305.1\n",
      "pywin32-ctypes                0.2.0\n",
      "pywinpty                      2.0.10\n",
      "PyYAML                        6.0\n",
      "pyzmq                         23.2.0\n",
      "QDarkStyle                    3.0.2\n",
      "qstylizer                     0.2.2\n",
      "QtAwesome                     1.2.2\n",
      "qtconsole                     5.4.0\n",
      "QtPy                          2.2.0\n",
      "queuelib                      1.5.0\n",
      "regex                         2022.7.9\n",
      "requests                      2.28.1\n",
      "requests-file                 1.5.1\n",
      "requests-toolbelt             0.9.1\n",
      "rope                          1.7.0\n",
      "Rtree                         1.0.1\n",
      "ruamel.yaml                   0.17.21\n",
      "ruamel.yaml.clib              0.2.6\n",
      "ruamel-yaml-conda             0.17.21\n",
      "scikit-image                  0.19.3\n",
      "scikit-learn                  1.2.1\n",
      "scikit-learn-intelex          20230228.214818\n",
      "scipy                         1.10.0\n",
      "Scrapy                        2.8.0\n",
      "seaborn                       0.12.2\n",
      "Send2Trash                    1.8.0\n",
      "service-identity              18.1.0\n",
      "setuptools                    65.6.3\n",
      "sip                           6.6.2\n",
      "six                           1.16.0\n",
      "smart-open                    5.2.1\n",
      "sniffio                       1.2.0\n",
      "snowballstemmer               2.2.0\n",
      "sortedcontainers              2.4.0\n",
      "soupsieve                     2.3.2.post1\n",
      "Sphinx                        5.0.2\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.5\n",
      "spyder                        5.4.1\n",
      "spyder-kernels                2.4.1\n",
      "SQLAlchemy                    1.4.39\n",
      "stack-data                    0.2.0\n",
      "statsmodels                   0.13.5\n",
      "sympy                         1.11.1\n",
      "tables                        3.7.0\n",
      "tabulate                      0.8.10\n",
      "TBB                           0.2\n",
      "tblib                         1.7.0\n",
      "tenacity                      8.0.1\n",
      "terminado                     0.17.1\n",
      "text-unidecode                1.3\n",
      "textdistance                  4.2.1\n",
      "threadpoolctl                 2.2.0\n",
      "three-merge                   0.1.1\n",
      "tifffile                      2021.7.2\n",
      "tiktoken                      0.3.3\n",
      "tinycss2                      1.2.1\n",
      "tldextract                    3.2.0\n",
      "tokenizers                    0.11.4\n",
      "toml                          0.10.2\n",
      "tomli                         2.0.1\n",
      "tomlkit                       0.11.1\n",
      "toolz                         0.12.0\n",
      "torch                         1.12.1\n",
      "tornado                       6.1\n",
      "tqdm                          4.64.1\n",
      "traitlets                     5.7.1\n",
      "transformers                  4.24.0\n",
      "Twisted                       22.2.0\n",
      "twisted-iocpsupport           1.0.2\n",
      "typing_extensions             4.4.0\n",
      "ujson                         5.4.0\n",
      "Unidecode                     1.2.0\n",
      "urllib3                       1.26.14\n",
      "w3lib                         1.21.0\n",
      "watchdog                      2.1.6\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "websocket-client              0.58.0\n",
      "Werkzeug                      2.2.2\n",
      "whatthepatch                  1.0.2\n",
      "wheel                         0.38.4\n",
      "widgetsnbextension            3.5.2\n",
      "win-inet-pton                 1.1.0\n",
      "wincertstore                  0.2\n",
      "wrapt                         1.14.1\n",
      "xarray                        2022.11.0\n",
      "xlwings                       0.29.1\n",
      "yapf                          0.31.0\n",
      "zict                          2.1.0\n",
      "zipp                          3.11.0\n",
      "zope.interface                5.4.0\n",
      "zstandard                     0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04bf39d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b6fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd362bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "openai.api_key = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526f17c",
   "metadata": {},
   "source": [
    "Test different promp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ec67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \\\"Unknown\\\". Q: Who is the greatest investor of all time?\\n\\n\"\n",
    "engine = 'ada'\n",
    "response = openai.Completion.create(\n",
    "  engine=engine, \n",
    "  prompt=prompt,\n",
    "  temperature=0.3, # The temperature controls the randomness of the response, represented as a range from 0 to 1. A lower value of temperature means the API will respond with the first thing that the model sees; a higher value means the model evaluates possible responses that could fit into the context before spitting out the result.\n",
    "  max_tokens=140,\n",
    "  top_p=1, # Top P controls how many random results the model should consider for completion, as suggested by the temperature dial, thus determining the scope of randomness. Top P’s range is from 0 to 1. A lower value limits creativity, while a higher value expands its horizons.\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=1\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96705a06",
   "metadata": {},
   "source": [
    "**bold text**# Summarization Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fa7c99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Operator\\n\\nGood afternoon. My name is Emma, and I will be your conference operator today. At this time, I would like to welcome everyone to the NVIDIA's third quarter earnings call. [Operator instructions] Simona Jankowski, you may begin your conference.\\n\\nSimona Jankowski -- Vice President, Investor Relations\\n\\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2023. With me today from NVIDIA are Jen-Hsun Huang, president and chief executive officer; and Colette Kress, executive vice president and chief financial officer. I'd like to remind you that our call is being webcast live on NVIDIA's investor relations website.\\n\\n\\nThe webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2023. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations.\\n\\nThese are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release our most recent Forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, November 16, 2022, and based on information currently available to us. Except as required by law, we assume no obligation to update any such statements.\\n\\nDuring this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\n\\nThanks, Simona. Q3 revenue was $5.93 billion, down 12% sequentially and down 17% year on year. We delivered record data center and automotive revenue. while our gaming and pro visualization platforms declined as we work through channel inventory corrections and challenging external conditions.\\n\\nStarting with data center. Revenue of $3.83 billion was up 1% sequentially and 31% year-on-year. This reflects very solid performance in the face of macroeconomic challenges new export controls and lingering supply chain disruptions. Year-on-year growth was driven primarily by leading U.S.\\n\\ncloud providers and a broadening set of consumer Internet companies for workloads such as large language models, recommendation systems and generative AI. As the number and scale of public cloud computing and Internet service companies deploying NVIDIA AI grows our traditional hyperscale definition will need to be expanded to convey the different end market use cases. We will align our data center customer commentary going forward accordingly. Other vertical industries, such as automotive and energy, also contributed to growth with key workloads relating to autonomous driving, high-performance computing, simulations and analytics.\\n\\nDuring the quarter, the U.S. government announced new restrictions impacting exports of our A100 and H-100 based products to China, and any product destined for certain systems or entities in China. These restrictions impacted third quarter revenue, largely offset by sales of alternative products into China. That said, demand in China more broadly remains soft, and we expect that to continue in the current quarter.\\n\\n\\nWe started shipping our flagship 100 data center GPU based on the new hopper architecture in Q3. A100-based systems are available starting this month from leading server makers including Dell, Hewlett Packard Enterprise, Lenovo and SuperMicro. Early next year, the first H-100 based cloud instances will be available on Amazon Web Services, Google Cloud, Microsoft Azure and Oracle Cloud Infrastructure. A100 delivered the highest performance and workload versatility for both AI training and inference in the latest MLPerf industry benchmarks.\\n\\nH-100 also delivers incredible value compared to the previous generation for equivalent AI performance it offers three x lower total cost of ownership while using five x fewer server nodes and 3.5 x less energy. Earlier today, we announced a multiyear collaboration with Microsoft to build an advanced cloud-based AI supercomputer to help enterprises train, deploy and scale AI including large state-of-the-art models. MacBook Azure will incorporate our complete AI stack, adding tens and thousands of A100 and A100 GPUs. Quantum 2 400 gigabit per second InfiniBand networking and the NVIDIA AI enterprise software suite to its platform.\\n\\nOracle and NVIDIA are also working together to offer AI training and inference at scale to thousands of enterprises. This includes bringing to Oracle Cloud infrastructure, the full NVIDIA accelerated computing stack and adding tens of thousands of NVIDIA GPUs, including the A100 and H-100. Cloud-based high-performance in the company, new scale is adopting NVIDIA AI enterprise and other software to address the industrial scientific communities, rising demand for AI in the cloud. NVIDIA AI will bring new capability to rescale high-performance computing as a service offerings, which include simulation and engineering software used across industries.\\n\\nNetworking posted strong growth driven by hyperscale customers and easing supply constraints. -- our new Quantum 240 gigabit per second InfiniBand and Spectrum Ethernet networking platforms are building momentum. We achieved an important milestone this quarter with VMware. And whose leading server virtualization platform, vSphere, has been rearchitected over the last two years to run on DPUs and now supports our BlueField DPUs.\\n\\nOur joint enterprise AI platform is available first on Dell PowerEdge servers. The BlueField DPU design win pipeline is growing and the number of infrastructure softer partners is expanding, including Arista, Check Point, Juniper, [Inaudible] Networks and Red Hot. The latest top 500 list of supercomputers released this week at Supercomputing '22 and has the highest ever number of NVIDIA-powered systems, including 72% of the total and 90% of new systems on the list. Moreover, NVIDIA powers 23 of the top 30 of the Green 500 list, demonstrating the energy efficiency of accelerated computing.\\n\\nThe No. 1 most energy-efficient system is the Flat Iron Institute Henry, which is the first top 500 system featuring our H-100 GPUs. At GTC, we announced the NVIDIA Omniverse Computing System, or OVS, reference designs featuring the new L4 GPU based on the ADA Lovelace architecture. These systems are designed to build and operate 3D virtual world using NVIDIA Omniverse enterprise.\\n\\nNVIDIA OBX systems will be available from Inspur, Lenovo and Super Micro by early 2023. We Lockheed Martin and Jaguar Land Rover will be among the first customers to receive OVS systems. We are further expanding our AI software and services offerings with NVIDIA and Bio Nemo large language model services, which are both entering early access this month. These enable developers to easily adopt large language models and deploy customized AI applications for content generation, tech summarization, chatbox, co-development, protein structure and biomolecular property predictions.\\n\\nMoving to gaming. Revenue of $1.57 billion was down 23% sequentially and down 51% from a year ago, reflecting lower sell-in to partners to help align channel inventory levels with current demand expectations. We believe Channel inventories are on track to approach normal levels as we exit Q4. Sell-through for our gaming products was relatively solid in the Americas and EMEA and but softer in Asia Pac as macroeconomic conditions and covered lockdowns in China continued to weigh on consumer demand.\\n\\nOur new Ada Lovelace GPU architecture had an exceptional launch. The first ADA GPU, the GeForce RTX 4090 became available in mid-October and a tremendous amount and positive feedback from the gaming community. We sold out quickly in many locations and are working hard to keep up with demand. The next member of the ATA family, RTX 4080 is available today.\\n\\nThe RTX 40 Series GPUs features DLSS 3, the neuro rendering technology that uses AI to generate entire frames for faster game play. Our third-generation RTX technology has raised the bar for computer graphics and help supercharge gaming. For example, the 15-year old classic game portal, now reimagined with full ray tracing and DLSS 3 has made it on Steam's top 100 most wish-listed gains. The total number of RTX games and applications now exceeds 350.\\n\\nThere is tremendous energy in the gaming community that we believe will continue to fuel strong fundamentals over the long term. The number of simultaneous users on steam just hit a record of $30 million, surpassing the prior peak of $28 million in January. Activision's Call of Duty Modern Warfare 2 set a record for the franchise with more than $800 million in opening weekend sales. topping the combined box office openings of movie blockbusters, TopGun Maverick and Dr.\\n\\nStrains in the Multiverse of [Inaudible]. And this month's League of Legends World Championship in San Francisco sold out minutes with 18,000 esports fans packed the arena where the Golden State Warriors play. We continue to expand the GeForce NOW cloud gaming service. In Q3, we added over 85 games to the library, bringing the total to over 1,400.\\n\\nWe also launched GeForce now on the new gaming devices, including Logitech, Cloud handheld, cloud gaming Chromebooks and Razor 5G Edge. Moving to Probi Revenue of $200 million was down 60% sequentially and down 65% from a year ago, reflecting lower sell-in to partners to help align channel inventory levels with the current demand expectations. These dynamics are expected to continue in Q4. Despite near-term challenges, we believe our long-term opportunity remains intact, fueled by AI simulation, computationally intensive design and engineering workloads.\\n\\nAt GTC, we announced NVIDIA Omniverse Cloud Services, our first software and infrastructure as a service offering, enabling artists, developers and enterprise teams to design, publish and operate metaverse applications from anywhere on any device. Omniverse Cloud Services runs on Omniverse cloud computer, a computing system comprised of NVIDIA OBX for graphics and physics simulation. NVIDIA HDX for AI workloads and the NVIDIA graphics delivery network, a global scale, distributed data center network for delivering low-latency metaverse graphics on the edge. Leaders in some of the world's largest industries continue to adopt Omniverse.\\n\\nHome improvement retailer, Lowe's is using it to help design, build and operate digital twins for their stores. Charter Communications and advanced analytics company, heavy AI are creating Omniverse power digital twins to optimize Charter's wireless network. In Deutsche Bahn, operator of German National Railway is using Omniverse to create digital twins of its rail network and train AI models to monitor the network, increasing safety and reliability. Moving to automotive.\\n\\nRevenue of $251 million, increased 14% sequentially and 86% from a year ago. Growth was driven by an increase in AI automotive solutions as our customers drive or on-based production ramp, continue to scale. Automotive has great momentum and is on its way to be our next multibillion-dollar platform. Global cars unveiled the all-new flagship Volvo EX90 SUV powered by the NVIDIA Drive platform.\\n\\nThis is the first model to use Volvo's software-defined architecture with a centralized core computer containing both drive Orin and DRIVEXaviar, along with 30 sensors. Other recently announced design wins and new model introductions include ton, auto, Neo, Polystar and [Inaudible]. At GTC, we also announced that NVIDIA Drive Super Chip, the successor to Orin in our automotive SoC road map, drive [Inaudible] delivers up to 2,000 tariff lots of performance and leverages technologies introduced in our Grace Hopper and ADA architectures. It is capable of running both the automated drive and in-vehicle infotainment systems.\\n\\nSimultaneously offering a LIFA performance while reducing cost and energy consumption. Driver will be available for automakers 25 models with Geely owned automaker, Zika as the first announced customer. Moving to the rest of the P&L. GAAP gross margin was 53.6% and and non-GAAP gross margin was 56.1%.\\n\\nGross margins reflect $702 million in inventory charges largely related to lower data center demand in China, partially offset by a warranty benefit of approximately $70 million. Year-on-year, GAAP operating expenses were up 31%, and non-GAAP operating expenses were up 30%, primarily due to higher compensation expenses related to headcount growth and salary increases and higher data center infrastructure expenses. Sequentially, both GAAP and non-GAAP operating expense growth was in the single-digit percent, and we plan to keep it relatively flat at these levels over the coming quarters. We returned $3.75 billion to shareholders in the form of share repurchases and cash dividends.\\n\\nAt the end of Q3, we had approximately $8.3 billion remaining under our share repurchase authorization through December 23. Let me turn to the outlook for the fourth quarter of fiscal 2023. We expect our data center revenue to reflect early production shipments of the A100, offset by continued softness in China. In gaming, we expect to resume sequential growth with our revenue still below end demand as we continue to work through the channel inventory correction.\\n\\nAnd in automotive, we expect the continued ramp of our Oren design wins. All in, we expect modest sequential growth driven by automotive, gaming and data center. Revenue is expected to be $6 billion, plus or minus 2%. GAAP and non-GAAP gross margins are expected to be $63.2 million and 66%, respectively, plus or minus 50 basis points.\\n\\nGAAP operating expenses are expected to be approximately $2.56 billion. Non-GAAP operating expenses are expected to be approximately $1.78 billion. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $40 million, excluding gains and losses on nonaffiliated investments. GAAP and non-GAAP tax rates are expected to be 9%, plus or minus 1%, excluding any discrete items.\\n\\nCapital expenditures are expected to be approximately $500 million to $550 million. Further financial details are included in the CFO commentary and other information available on our IR website. In closing, let me highlight upcoming events for the financial community. We'll be attending the Credit Suisse conference in Phoenix on November 30.\\n\\nThe rate Virtual Tech Conference on December 5 and and the JPMorgan Forum on January 5 in Las Vegas. Our earnings call to discuss the results of our fourth quarter and fiscal 2023 are scheduled for Wednesday, February 22. We will now open the call for questions. Operator, could you please poll for questions?\\n\\nQuestions & Answers:\\nOperator\\n\\n[Operator instructions] Your first question comes from the line of Vivek Arya with Bank of America Securities. Your line is now open.\\n\\nVivek Arya -- Bank of America Merrill Lynch -- Analyst\\n\\nThanks for taking my question. Colette, just wanted to clarify first, I think last quarter, you gave us a sell-through rate for your gaming business at about $2.5 billion a quarter. I think you said China is somewhat weaker. So I was hoping you could update us on what that sell-through rate is right now for gaming.\\n\\nAnd then, Jen-Hsun, the question for you. A lot of concerns about large hyperscalers cutting their spending and pointing to a slowdown. So if, let's say, U.S. cloud capex is flat or slightly down next year, do you think your business can still grow in the data center and why?\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nYes. Thanks for the question. Let me first start with the sell-through on our gaming business. we had indicated, if you put two quarters together, we would see approximately $5 billion in normalized sell-through for our business.\\n\\nNow, during the quarter, sell-through in Q3 three was relatively solid. We've indicated that although China lockdowns continue to channel -- excuse me, challenge our overall China business. It was still relatively solid. Notebook sell-through was also quite solid.\\n\\nAnd desktop, a bit softer, particularly in that China and Asia areas. We expect though stronger end demand, though, as we enter into Q4, driven by the upcoming holidays, as well as the continuation of the ADA adoption.\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nVivek, our data center business is indexed to two fundamental dynamics. The first has to do with general purpose computing no longer scaling. And so, acceleration is necessary to achieve the necessary level of cost efficiency scale and energy efficiency scale so that we can continue to increase workloads while saving money and saving power. Accelerated computing is recognized generally as the path forward as general purpose computing slows.\\n\\nThe second dynamic is AI. And we're seeing surging demand in some very important sectors of AIs in important breakthroughs in AI. One is called deep recommender systems, which is quite essential now to the best content or item or product to recommend to somebody who's using a device that is like a selfie or interacting with a computer just using voice. You need to really understand the nature, the context of the person making the request and make the appropriate recommendation to them.\\n\\nThe second has to do with large language models. This is -- this started several years ago with the invention of the transformer, which led to Bert, which led to GP3, which led to a whole bunch of other models now associated with that. We now have the ability to learn representations of languages of all kinds. It could be human language.\\n\\nIt could be the language of biology. It could be the language of chemistry. And recently, I just saw a breakthrough called Jeans LM, we just one of the first example of learning the language of human genomes. The third has to do with generative AI.\\n\\nYou know that the first 10 years, we've dedicated ourselves to perception AI. But the goal of perception, of course, is to understand context. But the ultimate goal of AI is to make a contribution to create something to generate product. And this is now the beginning of the era of generative AI.\\n\\nYou probably see it all over the place, whether they're generating images or generating videos or generating text of all kinds and the ability to augment our performance to enhance our performance to make productivity enhanced to reduce cost and improve whatever we do with whatever we have to work with, productivity is really more important than ever. And so, you could see that our company is indexed to two things, both of which are more important than ever, which is power efficiency, cost efficiency and then, of course, productivity. And these things are more important than ever. And my expectation is that we're seeing all the strong demand and surging demand for AI and for niche reasons.\\n\\nOperator\\n\\nYour next question comes from the line of C.J. Muse with Evercore. Your line is now open.\\n\\nC.J. Muse -- Evercore ISI -- Analyst\\n\\nYeah, Good afternoon and thank you for taking the question. You started to bundle on NVIDIA enterprise now with the H-100. I'm curious if you can talk about how we should think about timing around software monetization? And how we should kind of see this flow through the model, particularly with the focus on the AI enterprise and Omnivere side of things?\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nYes. Thanks, CJ. We're making excellent progress in NVIDIA AI enterprise. In fact, you saw probably that we made several announcements this quarter associated with clouds.\\n\\nYou know that NVIDIA has a rich ecosystem. And over the years, our rich ecosystem and our software stack has been integrated into developers and start-ups of all kinds, but more so -- more than ever, we're at the tipping point of clouds, and that's fantastic. Because if we could get NVIDIA's architecture and our full stack into every single cloud, we could reach more customers more quickly. And this quarter, we announced several initiatives, one has several partnerships and collaborations, one that we announced today, which has to do with Microsoft and our partnership there.\\n\\nIt has everything to do with scaling up AI because we have so many start-ups clamoring for large installations of our GPU so that they could do large language model training and building their start-ups and scale out of AI to enterprise and all of the world's Internet service providers. Every company we're talking to would like to have the agility and the scale, flexibility of clouds. And so, over the last year or so, we've been working on moving all of our software stacks to the cloud are of our platform and software stacks to the cloud. And so, today, we announced that Microsoft and ourselves are going to standardize on the NVIDIA stack, for a very large part of the work that we're doing together so that we could take a full stack out to the world's enterprise.\\n\\nThat's all software included. We, a month ago, announced the same similar type of partnership with Oracle. You also saw that rescale a leader in high-performance computing cloud has integrated NVIDIA AI into their stack. [Inaudible] has been integrated into GCP.\\n\\nAnd we announced recently Nemo, large language model and bionemo large language model to put NVIDIA software in the cloud. And we also announced Omniverse is now available in the cloud. The goal of all of this is to move the NVIDIA platform full stack off boarding the cloud so that we can engage customers much, much more quickly and customers could engage our software if they would like to use it in the cloud, it's per GPU instance hour if they would like to utilize our software on-prem, they could do it through software license. And so, license and subscription.\\n\\nAnd so, in both cases, we now have software available practically everywhere you would like to engage it. The partners that we work with are super excited about it because MBDA's rich ecosystem is global, and this could bring both new consumption into their cloud for both them and ourselves, but also connect all of these new opportunities to the other APIs and other services that they offer. And so, our software stack is making really great progress.\\n\\nOperator\\n\\nYour next question comes from the line of Chris Caso with Credit Suisse. Your line is now open.\\n\\nChris Caso -- Credit Suisse -- Analyst\\n\\nYes. Thank you. Good evening. I wonder if you could give some more color about the inventory charges you took in the quarter and then internal inventory in general.\\n\\nIn the documentation, you talked about that being a portion of inventory on hand plus some purchase obligations. And you also spoke in your prepared remarks that some of this was due to China data centers. So if you can clarify what was in those charges. And then, in general, for your internal inventory.\\n\\nDoes that still need to be worked down? And what are the implications if that needs to be worked down over the next couple of quarters?\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nThanks for the question, Chris. So as we highlighted in our prepared remarks, we booked an entry of $702 million for inventory reserves within the quarter. Most of that, primarily, all of it is related to our data center business, just due to the change in expected demand looking forward for China. So when we look at the data center products, a good portion of this was also the A100, which we wrote down.\\n\\nNow, looking at our inventory that we have on hand and the inventory that has increased, a lot of that is just due to our upcoming architectures coming to market. our ADA architecture, our hopper architecture and even more in terms of our networking business. We have been building for those architectures to come to market and as such to say. We are always looking at our inventory levels at the end of each quarter for our expected demand going forward.\\n\\nBut I think we've done a solid job that we used in this quarter just based on that expectation going forward.\\n\\nOperator\\n\\nYour next question comes from the line of Timothy Arcuri with UBS. Your line is now open.\\n\\nTimothy Arcuri -- UBS -- Analyst\\n\\nThanks a lot. Colette, can you -- I have a two-part question. First, is there any effect of stockpiling in the data center guidance? I ask because you now have the A800 that is sort of a modified version of the A100 with the lower data transfer rate. So one could imagine that customers might be stocking that while they can still get it.\\n\\nAnd I guess the second part of that is related to the inventory charge, can you just go into that a little bit more? Because last quarter, it made sense that you took a charge because revenue was less than you thought, but revenue came in pretty much in line. And it sounded like China was a net neutral. So is the charge related to just working A100 inventory down faster? Is that what the charges related to?\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nSure. So let me talk about the first statement that you indicated. Most of our data center business that we see is we're working with customers specifically on their needs to build out accelerated computing and AI. It's just not a business in terms of where units are being held for that.\\n\\nThey're usually four very, very specific products and projects that we see. So I'm going to answer no. Nothing that we can see. Your second question regarding the inventory provisions.\\n\\nAt the end of last quarter, we were beginning to see softness in China. We've always been looking at our needs long term. It's not a statement about the current quarter in inventory, as you can see. It usually takes two or three quarters for us to build product for the future demand.\\n\\nSo that's always a case of the inventory that we are ordering. So now looking at what we've seen in terms of continued lockdowns, continued economy challenges in China it was time for us to take a hard look of what do we think we'll need for data center going forward and not leg for write-downs.\\n\\nOperator\\n\\nYour next question comes from the line of Stacy Rasgon with Bernstein. Your line is now open.\\n\\nStacy Rasgon -- AllianceBernstein -- Analyst\\n\\nHi, guys. Thanks for taking my question. Colette, I had a question on the commentary you gave on the sequentials. It kind of sounded like data center maybe had some China softness issues.\\n\\nYou said gaming resumed sequential growth. But then you said sequential growth for the company driven by auto gaming and data center. How can all three of those grow sequentially if the overall guidance is kind of flattish? Are they all just like growing just a little bit? Or is one of them actually down? Like how do we think about the segments into Q4 given that commentary?\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nYes. So your question is regarding the sequentials from Q3 to our guidance that we provided for Q4. As we are seeing the numbers in terms of our guidance, you're correct, is only growing about $100 million. And we've indicated that three of those platforms will likely grow just a little bit.\\n\\nBut our pro visualization business we think is going to be flattish and likely not growing as we're still working on correcting the channel inventory levels. to get to the right amount. It's very difficult to say which will have that increase. But again, we are planning for all three of those different market platforms to grow just a little bit.\\n\\nOperator\\n\\nYour next question comes from the line of Mark Lipacis with Jefferies. Your line is now open.\\n\\nMark Lipacis -- Jefferies -- Analyst\\n\\nHi. Thanks for taking my question. Jen-Hsun, I think for you, you've articulated a vision for the data center we're a solution with an integrated solution set of a CPU, GPU and DPU is deployed for all workloads or most workloads, I think. Could you just give us a sense of or talk about where is this vision in the penetration cycle? And maybe talk about Grace Grace's importance for realizing that vision, what will Grace deliver versus an off-the-shelf x86 where -- do you have a sense of where Grace will get embraced first or the fastest within that vision?\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nGrace's data moving capability is off the charts. Grace also is memory coherent to our GPU, which allows our GPU to expand its effective GPU memory, fast GPU memory by a factor of 10. That's not possible without special capabilities that are designed between hopper and Grace and the architecture of Grace. And so, it was designed.\\n\\nGrace is designed for very large data processing at very high speeds. Those applications are related to, for example, data processing is related for recommender systems, which operates on petabytes of live data at a time. It's all hot. It all needs to be fast, so that you can make a recommendation within milliseconds to hundreds of millions of people using our service.\\n\\nIt is also quite effective at AI training, machine learning. And so, those kind of applications are really terrific. We -- Grace, I think I've said before that we will have production samples in Q1, and we're still on track to do that.\\n\\nOperator\\n\\nYour next question comes from the line of Harlan Sur with J.P. Morgan. Your line is now open.\\n\\nHarlan Sur -- JPMorgan Chase and Company -- Analyst\\n\\nGood afternoon and thanks for taking my question. Your data center networking business, I believe, is driving about $800 million per quarter in sales, very, very strong growth over the past few years. Near term, as you guys pointed out, and the team is driving strong Nick and blue food attached to your own compute solutions like DGX and more partner announcements like VMware, but we also know that networking has pretty large exposure to general purpose cloud and hyperscale compute spending trends. So what's the visibility and growth outlook for the networking business over the next few quarters?\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nYes. If I could take that. First, thanks for your question. Our networking, as you know, is heavily indexed to high-performance computing.\\n\\nWe're not -- we don't serve the vast majority of commodity networking. All of our network solutions are very high end, and they're designed for data centers that move a lot of data. Now, if you have a hyperscale data center these days, and you are deploying a large number of AI applications. It is very likely that the network bandwidth that you provision has a substantial implication on the overall throughput of your data center.\\n\\nSo the small incremental investment they make in high-performance networking translates to billions of dollars of savings slightly in provisioning the service or billions of dollars more throughput, which increases their economics. And so, these days, with disaggregated and I application, AI provisioning and data centers, high-performance networking is really quite fantastic and it pays for itself right away. But that's where we are focused in high-performance networking and provisioning AI services in -- well, the AI applications that we focus on. You might have noticed that NVIDIA and Microsoft are building one of the largest AI infrastructures in the world.\\n\\nAnd it is completely powered by NVIDIA's InfiniBand 400 gigabits per second network. And the reason for that is because that network pays for itself instantaneously. The investment that you're going to put into the infrastructure is so significant that if you were to be dragged by slow networks, obviously, the efficiency of the overall infrastructure is not as high. And so, in the places where we focus networking is really quite important.\\n\\nIt goes all the way back to when we first announced the acquisition of Mellanox. I think at the time, they were doing about a few hundred million dollars a quarter, about $400 million a quarter. And now we're doing what they used to do in the old days, in a year, practically coming up in a quarter. And so, that kind of tells you about the growth of high-performance networking.\\n\\nIt is an indexed to overall enterprise and data center spend but it is highly indexed to AI adoption.\\n\\nOperator\\n\\nYour next question comes from the line of Aaron Rakers with Wells Fargo. Your line is now open.\\n\\nAaron Rakers -- Wells Fargo Securities -- Analyst\\n\\nThanks for taking the question. I want to expand on the networking question a little bit further. When we look at the Microsoft announcement today, we think about what Meda is doing on the AI footprint that they're deploying. Jen-Hsun, can you help us understand like where your InfiniBand networking sits relative to like traditional data center switching? And maybe kind of build on that, how you're positioning spectrum for in the market, does that compete against a broader set of opportunities in the Ethernet world for AI fabric networking?\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nYes. Thanks, Erin. The math is like this. If you're going to spend $20 billion on an infrastructure and the efficiency of that overall data center is improved by 10%.\\n\\nThe numbers are huge. And when we do these large language models and recommender systems, the processing is done across the entire data center. And so, we distribute the workload across multiple GPUs, multiple nodes and it runs for a very long time. And so, the importance of the network can be overemphasized.\\n\\nAnd so, the difference of 10% in overall improvement in efficiency, which is very to achieve. The difference between NVIDIA's InfiniBand, the entire software stack with what we call Magnum IO, which allows us to do computing in the network itself. A lot of software is running in the network itself, not just moving data around. We call it in-network computing because a ton of software is done at the edge at the -- within the network itself.\\n\\nWe achieved significant differences in overall efficiency. And so, if you're spending billions of dollars on the infrastructure, or even hundreds of millions of dollars of interest on the infrastructure. The difference is really quite profound.\\n\\nOperator\\n\\nYour next question comes from the line of Ambrish Srivastava with BMO. Your line is now open.\\n\\nAmbrish Srivastava -- BMO Capital Markets -- Analyst\\n\\nHi. Thank you very much. I actually had a couple of clarifications. Colette, in the data center side, is it a fair assumption that compute was down Q-over-Q in the reported quarter because the quarter before, Mellanox or the networking business was up as it was called out.\\n\\nAnd again, you said it grew quarter over quarter. So is that a fair assumption? And then, I had a clarification on the USG band. Initially, it was supposed to be a $400 million, really going to what the government was trying to firewall. Is the A800 -- I'm just trying to make sure I understand it.\\n\\nIsn't that against the spirit of what the government is trying to do, i.e., firewall, high-performance compute? Or is A800 going to a different set of customers?\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nThank you for the question. So looking at our compute for the quarter is about flattish. Yes, we're seeing also growth growth in terms of our networking, but you should look at our Q3 compute is about flatters with last quarter.\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nAmbrish, A800 hardware, the hardware of ensures that it always meets U.S. government's clear test for export control. And it cannot be customer reprogrammed or application reprogrammed to exceed it. It is hardware limited.\\n\\nIt is in the hardware that determines 800s capabilities. And so, it meets the clear test in letter and in spirit. We raised the concern about the $400 million of A100s because we were uncertain about whether we could execute. The introduction of A800 to our customers and through our supply chain in time.\\n\\nThe company did remarkable feeds to swarm this situation and make sure that our business was not affected and our customers were not affected. But A800 hardware surely ensures that it always meets U.S. government's clear tests for export control.\\n\\nOperator\\n\\nYour next question comes from the line of William Stein with Truist Securities. Your line is now open.\\n\\nWilliam Stein -- Truist Securities -- Analyst\\n\\nThank you. I'm hoping you can discuss the pace of 100 growth as we progress over the next year. We've gotten a lot of questions as to whether the ramp in this product should look like a sort of traditional product cycle where there's quite a bit of pent-up demand for this significant improved performance product and that there's supply available as well. So does this rollout sort of look relatively typical from that perspective? Or should we expect a more perhaps delayed start of the growth trajectory where we see maybe substantially more growth in, let's say, second half of '23.\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nH-100 ramp is different than the A100 ramp in several ways. The first is that the TCO, the cost benefits, the operational cost benefits because of the energy savings because every data center is now Power Limited. And because of this incredible transformer engine that's designed for the latest AI models. The performance over Ampere is so significant that I -- and because of the pent-up demand for hopper because of these new models that are that I spoke about earlier, deep recommender systems and large language models and generative AI models.\\n\\nCustomers are clamoring to ramp hopper as quickly as possible, and we are trying to do the same. We are all hands on deck to help the cloud service providers stand up the supercomputers. Remember, I is the only company in the world that produces and ships semi-custom supercomputers in high volume. It's a miracle to ship one supercomputer every three years.\\n\\nit's unheard of to ship supercomputers to every cloud service provider in a quarter. And so, we're working hand with every one of them, and every one of them are racing to stand up hoppers. We expect them to have hopper cloud services stood up in Q1. And so, we are expecting to ship some volume, we're expecting to ship production in Q4, and then we're expecting to ship large volumes in Q1.\\n\\nThat's a faster transition than MPIR. And so, it's because of the dynamics that I described.\\n\\nOperator\\n\\nYour next question comes from the line of Matt Ramsay with Cowen. Your line is now open.\\n\\nMatt Ramsay -- Cowen and Company -- Analyst\\n\\nYeah. Thank you very much. Good afternoon. I guess, Colette, I heard in your script that you had talked about maybe a new way of commenting on or reporting hyperscaler revenue in your data center business.\\n\\nAnd I wonder if you could maybe give us a little bit more detail about what you're thinking there and what sort of drove the decision? And I guess the derivative of that, Jen-Hsun, how -- that decision to talk about the data center business to hyperscalers differently. I mean, what does that mean for the business that is just a reflection of where demand is and you're going to break things out differently? Or is something changing about the mix of I guess, internal properties versus vertical industry demand within the hyperscale customer base.\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nYes, Matt, thanks for the question. Let me clarify a little bit in terms of what we believe we should be looking at when we go forward and discussing our data center business. Our data center business is becoming larger and larger and our customers are complex. And when we talk about hyperscale, we tend to talk about seven, eight different companies.\\n\\nBut the reality is there's a lot of very large companies that we could add to that discussion based on what they're purchasing. Additionally, looking at the cloud, looking at our cloud purchases and what our customers are building for the cloud is an important area to focus on because this is really where our enterprise is where our researchers, where our higher education is also purchasing. So we're trying to look for a better way to describe the color of what we're seeing in the cloud and also give you a better understanding of some of these large installments that we're seeing in the hyperscales.\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nYes. Let me double click on what Colette just said, which is absolutely right. There are two major dynamics that's happening. First, the adoption of NVIDIA in Internet service companies around the world, the number and the scale by which they're doing it has grown a lot.\\n\\nInternet service companies. And these are Internet service companies that offer services, but they're not public cloud computing companies. The second factor has to do with cloud computing. We are now at the tipping point of cloud computing.\\n\\nAlmost every enterprise in the world has both a cloud-first and a multi-cloud strategy. It is exactly the reason why all of the announcements that we made this year -- this quarter, this last quarter since GTC about all the new platforms that are now available in the cloud. a CSP, a hyperscaler is both -- are two things to us, therefore, a hyperscaler can be a sell to customer. They are also a cell with partner on the public cloud side of their business.\\n\\nBecause of the richness of NVIDIA's ecosystem because we have so many Internet service customers and enterprise customers using NVIDIA's full stack. The public cloud side of their business really enjoys and values the partnership with us and the cell with relationship they have with us. And it's pretty clear now that for all of the hyperscalers, the public cloud side of their business will likely would very likely be the vast majority of their overall consumption. And so, because the world CSPs, the world's public clouds is only at the early innings of their enterprise to lifting enterprise to the cloud world it's very, very clear that the public cloud side of the business is going to be very large.\\n\\nAnd so, increasingly, our relationship with CSPs, our relationship with hyperscalers will -- will include, of course, continuing to sell to them for internal consumption but very importantly, sell with for the public cloud side.\\n\\nOperator\\n\\nYour next question comes from the line of Joseph Moore with Morgan Stanley. Your line is now open.\\n\\nJoseph Moore -- Morgan Stanley -- Analyst\\n\\nGreat. Thank you. I wonder if you could talk to looking backward at the crypto impact. Obviously, that's gone from your numbers now, but do you see any potential for liquidation of GPUs that are in the mining network, any impact going forward? And do you foresee blockchain being an important part of your business at some point down the road?\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nWe don't expect to see blockchain being an important part of our business down the road. There is always a resell market. If you look at any of the major resell sites, eBay, for example, there are secondhand graphics cards for sale all the time. And the reason for that is because a 3090 that somebody bought today, is upgraded to a 4090 or 3090 by a couple of years ago, it was up are until 4090 today.\\n\\nThat 3090 could be sold to somebody and enjoyed it sold at the right price. And so, the volume of -- the availability of secondhand and used graphics cards has always been there. And the inventory is never zero. and when the inventory is larger than usual, like all supply demand, it would likely drift lower price and affect the lower ends of our market.\\n\\nBut my sense is that where we're going right now with ADA is targeting very clearly in the upper range, the top half of our market. And and early signs are, and I'm sure you're also seeing that the ADA launch was a home run. That we shipped a large volume of 4090s because as you know, we were prepared for it. And yet within minutes, they were sold out around the world.\\n\\nAnd so, the reception of 4090 and the reception of 4080 today has been off the charts. And that says something about the strength and the health and the vibrancy of the gaming market. So we're super enthusiastic about the ADA launch. We have many more ad products to come.\\n\\nOperator\\n\\nYour last question today comes from the line of Toshiya Hari with Goldman Sachs. Your line is now open.\\n\\nToshiya Hari -- Goldman Sachs -- Analyst\\n\\nGreat. Thank you so much for squeezing me in. I had two quick ones for Colette. On supply, I think there was some mixed messaging in your remarks.\\n\\nI think you talked about supply being a headwind at one point. And then, when you were speaking to the networking business, I think you talked about supply easing. So I was hoping you can kind of speak to supply if you're caught up to demand at this point. And then, secondly, just on stock-based compensation, pretty mundane topic I realize, but it is -- I think in the quarter, it was about $700 million.\\n\\nIt's becoming a bigger piece of your opex. So curious how we should be modeling that going forward.\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nSure. When we look at our supply constraints that we have had in the past, each and every quarter, this is getting better Networking was one of our issues probably a year ago, and it has taken us probably to this quarter. and next quarter to really see our supply improved so that we can support the pipeline that we have for our customers that are -- now that's our supply. We've also made a discussion regarding our customers, supply constraints, issues when setting up a data center, even getting data center capacity has been very difficult.\\n\\nAnd therefore, that challenges them in their purchasing decisions as they're still looking for certain parts of that supply chain to come through. So that hopefully clarifies what we were talking about regarding two areas of supply. In our stock-based compensation, what we'll see, it's very difficult to predict what our stock-based compensation would be when it arrives. We have provided to our incoming employees but also once a year to our employees, and it's a single date in terms of when that is priced.\\n\\nSo it's difficult to determine, but stock-based compensation is an important part of our employees' compensation and will continue to be. So we look at it from an overall compensation perspective. So up until now and when we do the focal, we'll see about the same size with a few additions for the reduced level of employee hiring that we have right now.\\n\\nOperator\\n\\nThank you. I will now turn the call back over to Jen-Hsun Huang for closing remarks.\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nThanks, everyone. We are quickly adapting to the macro environment. Correcting inventory levels, offering alternative products to data center customers in China and keeping our opex flat for the next few quarters. Our new platforms are off to a great start and formed the foundation for our resumed growth.\\n\\nMRTX is reinventing 3D graphics with ray tracing and AI. The launch of [Inaudible] is phenomenal. Gamers waited in long lines around the world, 4090 stocks sold out quickly. Hopper, with its revolutionary transformer engine is just in time to meet the surging demand for recommender systems, large language models and generative AI.\\n\\nNVIDIA networking is synonymous with the highest data center throughput and enjoying record results. Oren is the world's first computing platform designed for AI-powered autonomous vehicles and robotics and putting automotive on the road to be our next multibillion-dollar platform. These computing platforms run NVIDIA AI and NVIDIA Omniverse, software libraries and engines that help the companies build and deploy AI to products and services. we this pioneering work and accelerated computing is more vital than ever.\\n\\nLimited by business, general purpose commuting has slowed to a crawl just as AI demands more computing. Scaling through general purchase computing alone is no longer viable, both from a cost or power standpoint. Accelerated computing is the path forward. We look forward to updating you on our progress next quarter.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://gist.githubusercontent.com/hackingthemarkets/e664894b65b31cbe8993e02d25d26768/raw/618afe09d07979cc72911ce79634ab5d2cc19a54/nvidia-earnings-call.txt\"\n",
    "response = requests.get(url)\n",
    "transcript = response.text\n",
    "\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "889a996a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Operator\\n\\nGood afternoon. My name is Emma, and I will be your conference operator today. At this time, I would like to welcome everyone to the NVIDIA's third quarter earnings call. [Operator instructions] Simona Jankowski, you may begin your conference.\\n\\nSimona Jankowski -- Vice President, Investor Relations\\n\\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2023. With me today from NVIDIA are Jen-Hsun Huang, president and chief executive officer; and Colette Kress, executive vice president and chief financial officer. I'd like to remind you that our call is being webcast live on NVIDIA's investor relations website.\\n\\n\\nThe webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2023. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations.\\n\\nThese are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release our most recent Forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, November 16, 2022, and based on information currently available to us. Except as required by law, we assume no obligation to update any such statements.\\n\\nDuring this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\n\\nThanks, Simona. Q3 revenue was $5.93 billion, down 12% sequentially and down 17% year on year. We delivered record data center and automotive revenue. while our gaming and pro visualization platforms declined as we work through channel inventory corrections and challenging external conditions.\\n\\nStarting with data center. Revenue of $3.83 billion was up 1% sequentially and 31% year-on-year. This reflects very solid performance in the face of macroeconomic challenges new export controls and lingering supply chain disruptions. Year-on-year growth was driven primarily by leading U.S.\\n\\ncloud providers and a broadening set of consumer Internet companies for workloads such as large language models, recommendation systems and generative AI. As the number and scale of public cloud computing and Internet service companies deploying NVIDIA AI grows our traditional hyperscale definition will need to be expanded to convey the different end market use cases. We will align our data center customer commentary going forward accordingly. Other vertical industries, such as automotive and energy, also contributed to growth with key workloads relating to autonomous driving, high-performance computing, simulations and analytics.\\n\\nDuring the quarter, the U.S. government announced new restrictions impacting exports of our A100 and H-100 based products to China, and any product destined for certain systems or entities in China. These restrictions impacted third quarter revenue, largely offset by sales of alternative products into China. That said, demand in China more broadly remains soft, and we expect that to continue in the current quarter.\\n\\n\\nWe started shipping our flagship 100 data center GPU based on the new hopper architecture in Q3. A100-based systems are available starting this month from leading server makers including Dell, Hewlett Packard Enterprise, Lenovo and SuperMicro. Early next year, the first H-100 based cloud instances will be available on Amazon Web Services, Google Cloud, Microsoft Azure and Oracle Cloud Infrastructure. A100 delivered the highest performance and workload versatility for both AI training and inference in the latest MLPerf industry benchmarks.\\n\\nH-100 also delivers incredible value compared to the previous generation for equivalent AI performance it offers three x lower total cost of ownership while using five x fewer server nodes and 3.5 x less energy. Earlier today, we announced a multiyear collaboration with Microsoft to build an advanced cloud-based AI supercomputer to help enterprises train, deploy and scale AI including large state-of-the-art models. MacBook Azure will incorporate our complete AI stack, adding tens and thousands of A100 and A100 GPUs. Quantum 2 400 gigabit per second InfiniBand networking and the NVIDIA AI enterprise software suite to its platform.\\n\\nOracle and NVIDIA are also working together to offer AI training and inference at scale to thousands of enterprises. This includes bringing to Oracle Cloud infrastructure, the full NVIDIA accelerated computing stack and adding tens of thousands of NVIDIA GPUs, including the A100 and H-100. Cloud-based high-performance in the company, new scale is adopting NVIDIA AI enterprise and other software to address the industrial scientific communities, rising demand for AI in the cloud. NVIDIA AI will bring new capability to rescale high-performance computing as a service offerings, which include simulation and engineering software used across industries.\\n\\nNetworking posted strong growth driven by hyperscale customers and easing supply constraints. -- our new Quantum 240 gigabit per second InfiniBand and Spectrum Ethernet networking platforms are building momentum. We achieved an important milestone this quarter with VMware. And whose leading server virtualization platform, vSphere, has been rearchitected over the last two years to run on DPUs and now supports our BlueField DPUs.\\n\\nOur joint enterprise AI platform is available first on Dell PowerEdge servers. The BlueField DPU design win pipeline is growing and the number of infrastructure softer partners is expanding, including Arista, Check Point, Juniper, [Inaudible] Networks and Red Hot. The latest top 500 list of supercomputers released this week at Supercomputing '22 and has the highest ever number of NVIDIA-powered systems, including 72% of the total and 90% of new systems on the list. Moreover, NVIDIA powers 23 of the top 30 of the Green 500 list, demonstrating the energy efficiency of accelerated computing.\\n\\nThe No. 1 most energy-efficient system is the Flat Iron Institute Henry, which is the first top 500 system featuring our H-100 GPUs. At GTC, we announced the NVIDIA Omniverse Computing System, or OVS, reference designs featuring the new L4 GPU based on the ADA Lovelace architecture. These systems are designed to build and operate 3D virtual world using NVIDIA Omniverse enterprise.\\n\\nNVIDIA OBX systems will be available from Inspur, Lenovo and Super Micro by early 2023. We Lockheed Martin and Jaguar Land Rover will be among the first customers to receive OVS systems. We are further expanding our AI software and services offerings with NVIDIA and Bio Nemo large language model services, which are both entering early access this month. These enable developers to easily adopt large language models and deploy customized AI applications for content generation, tech summarization, chatbox, co-development, protein structure and biomolecular property predictions.\\n\\nMoving to gaming. Revenue of $1.57 billion was down 23% sequentially and down 51% from a year ago, reflecting lower sell-in to partners to help align channel inventory levels with current demand expectations. We believe Channel inventories are on track to approach normal levels as we exit Q4. Sell-through for our gaming products was relatively solid in the Americas and EMEA and but softer in Asia Pac as macroeconomic conditions and covered lockdowns in China continued to weigh on consumer demand.\\n\\nOur new Ada Lovelace GPU architecture had an exceptional launch. The first ADA GPU, the GeForce RTX 4090 became available in mid-October and a tremendous amount and positive feedback from the gaming community. We sold out quickly in many locations and are working hard to keep up with demand. The next member of the ATA family, RTX 4080 is available today.\\n\\nThe RTX 40 Series GPUs features DLSS 3, the neuro rendering technology that uses AI to generate entire frames for faster game play. Our third-generation RTX technology has raised the bar for computer graphics and help supercharge gaming. For example, the 15-year old classic game portal, now reimagined with full ray tracing and DLSS 3 has made it on Steam's top 100 most wish-listed gains. The total number of RTX games and applications now exceeds 350.\\n\\nThere is tremendous energy in the gaming community that we believe will continue to fuel strong fundamentals over the long term. The number of simultaneous users on steam just hit a record of $30 million, surpassing the prior peak of $28 million in January. Activision's Call of Duty Modern Warfare 2 set a record for the franchise with more than $800 million in opening weekend sales. topping the combined box office openings of movie blockbusters, TopGun Maverick and Dr.\\n\\nStrains in the Multiverse of [Inaudible]. And this month's League of Legends World Championship in San Francisco sold out minutes with 18,000 esports fans packed the arena where the Golden State Warriors play. We continue to expand the GeForce NOW cloud gaming service. In Q3, we added over 85 games to the library, bringing the total to over 1,400.\\n\\nWe also launched GeForce now on the new gaming devices, including Logitech, Cloud handheld, cloud gaming Chromebooks and Razor 5G Edge. Moving to Probi Revenue of $200 million was down 60% sequentially and down 65% from a year ago, reflecting lower sell-in to partners to help align channel inventory levels with the current demand expectations. These dynamics are expected to continue in Q4. Despite near-term challenges, we believe our long-term opportunity remains intact, fueled by AI simulation, computationally intensive design and engineering workloads.\\n\\nAt GTC, we announced NVIDIA Omniverse Cloud Services, our first software and infrastructure as a service offering, enabling artists, developers and enterprise teams to design, publish and operate metaverse applications from anywhere on any device. Omniverse Cloud Services runs on Omniverse cloud computer, a computing system comprised of NVIDIA OBX for graphics and physics simulation. NVIDIA HDX for AI workloads and the NVIDIA graphics delivery network, a global scale, distributed data center network for delivering low-latency metaverse graphics on the edge. Leaders in some of the world's largest industries continue to adopt Omniverse.\\n\\nHome improvement retailer, Lowe's is using it to help design, build and operate digital twins for their stores. Charter Communications and advanced analytics company, heavy AI are creating Omniverse power digital twins to optimize Charter's wireless network. In Deutsche Bahn, operator of German National Railway is using Omniverse to create digital twins of its rail network and train AI models to monitor the network, increasing safety and reliability. Moving to automotive.\\n\\nRevenue of $251 million, increased 14% sequentially and 86% from a year ago. Growth was driven by an increase in AI automotive solutions as our customers drive or on-based production ramp, continue to scale. Automotive has great momentum and is on its way to be our next multibillion-dollar platform. Global cars unveiled the all-new flagship Volvo EX90 SUV powered by the NVIDIA Drive platform.\\n\\nThis is the first model to use Volvo's software-defined architecture with a centralized core computer containing both drive Orin and DRIVEXaviar, along with 30 sensors. Other recently announced design wins and new model introductions include ton, auto, Neo, Polystar and [Inaudible]. At GTC, we also announced that NVIDIA Drive Super Chip, the successor to Orin in our automotive SoC road map, drive [Inaudible] delivers up to 2,000 tariff lots of performance and leverages technologies introduced in our Grace Hopper and ADA architectures. It is capable of running both the automated drive and in-vehicle infotainment systems.\\n\\nSimultaneously offering a LIFA performance while reducing cost and energy consumption. Driver will be available for automakers 25 models with Geely owned automaker, Zika as the first announced customer. Moving to the rest of the P&L. GAAP gross margin was 53.6% and and non-GAAP gross margin was 56.1%.\\n\\nGross margins reflect $702 million in inventory charges largely related to lower data center demand in China, partially offset by a warranty benefit of approximately $70 million. Year-on-year, GAAP operating expenses were up 31%, and non-GAAP operating expenses were up 30%, primarily due to higher compensation expenses related to headcount growth and salary increases and higher data center infrastructure expenses. Sequentially, both GAAP and non-GAAP operating expense growth was in the single-digit percent, and we plan to keep it relatively flat at these levels over the coming quarters. We returned $3.75 billion to shareholders in the form of share repurchases and cash dividends.\\n\\nAt the end of Q3, we had approximately $8.3 billion remaining under our share repurchase authorization through December 23. Let me turn to the outlook for the fourth quarter of fiscal 2023. We expect our data center revenue to reflect early production shipments of the A100, offset by continued softness in China. In gaming, we expect to resume sequential growth with our revenue still below end demand as we continue to work through the channel inventory correction.\\n\\nAnd in automotive, we expect the continued ramp of our Oren design wins. All in, we expect modest sequential growth driven by automotive, gaming and data center. Revenue is expected to be $6 billion, plus or minus 2%. GAAP and non-GAAP gross margins are expected to be $63.2 million and 66%, respectively, plus or minus 50 basis points.\\n\\nGAAP operating expenses are expected to be approximately $2.56 billion. Non-GAAP operating expenses are expected to be approximately $1.78 billion. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $40 million, excluding gains and losses on nonaffiliated investments. GAAP and non-GAAP tax rates are expected to be 9%, plus or minus 1%, excluding any discrete items.\\n\\nCapital expenditures are expected to be approximately $500 million to $550 million. Further financial details are included in the CFO commentary and other information available on our IR website. In closing, let me highlight upcoming events for the financial community. We'll be attending the Credit Suisse conference in Phoenix on November 30.\\n\\nThe rate Virtual Tech Conference on December 5 and and the JPMorgan Forum on January 5 in Las Vegas. Our earnings call to discuss the results of our fourth quarter and fiscal 2023 are scheduled for Wednesday, February 22. We will now open the call for questions. Operator, could you please poll for questions?\\n\\nQuestions & Answers:\\nOperator\\n\\n[Operator instructions] Your first question comes from the line of Vivek Arya with Bank of America Securities. Your line is now open.\\n\\nVivek Arya -- Bank of America Merrill Lynch -- Analyst\\n\\nThanks for taking my question. Colette, just wanted to clarify first, I think last quarter, you gave us a sell-through rate for your gaming business at about $2.5 billion a quarter. I think you said China is somewhat weaker. So I was hoping you could update us on what that sell-through rate is right now for gaming.\\n\\nAnd then, Jen-Hsun, the question for you. A lot of concerns about large hyperscalers cutting their spending and pointing to a slowdown. So if, let's say, U.S. cloud capex is flat or slightly down next year, do you think your business can still grow in the data center and why?\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nYes. Thanks for the question. Let me first start with the sell-through on our gaming business. we had indicated, if you put two quarters together, we would see approximately $5 billion in normalized sell-through for our business.\\n\\nNow, during the quarter, sell-through in Q3 three was relatively solid. We've indicated that although China lockdowns continue to channel -- excuse me, challenge our overall China business. It was still relatively solid. Notebook sell-through was also quite solid.\\n\\nAnd desktop, a bit softer, particularly in that China and Asia areas. We expect though stronger end demand, though, as we enter into Q4, driven by the upcoming holidays, as well as the continuation of the ADA adoption.\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nVivek, our data center business is indexed to two fundamental dynamics. The first has to do with general purpose computing no longer scaling. And so, acceleration is necessary to achieve the necessary level of cost efficiency scale and energy efficiency scale so that we can continue to increase workloads while saving money and saving power. Accelerated computing is recognized generally as the path forward as general purpose computing slows.\\n\\nThe second dynamic is AI. And we're seeing surging demand in some very important sectors of AIs in important breakthroughs in AI. One is called deep recommender systems, which is quite essential now to the best content or item or product to recommend to somebody who's using a device that is like a selfie or interacting with a computer just using voice. You need to really understand the nature, the context of the person making the request and make the appropriate recommendation to them.\\n\\nThe second has to do with large language models. This is -- this started several years ago with the invention of the transformer, which led to Bert, which led to GP3, which led to a whole bunch of other models now associated with that. We now have the ability to learn representations of languages of all kinds. It could be human language.\\n\\nIt could be the language of biology. It could be the language of chemistry. And recently, I just saw a breakthrough called Jeans LM, we just one of the first example of learning the language of human genomes. The third has to do with generative AI.\\n\\nYou know that the first 10 years, we've dedicated ourselves to perception AI. But the goal of perception, of course, is to understand context. But the ultimate goal of AI is to make a contribution to create something to generate product. And this is now the beginning of the era of generative AI.\\n\\nYou probably see it all over the place, whether they're generating images or generating videos or generating text of all kinds and the ability to augment our performance to enhance our performance to make productivity enhanced to reduce cost and improve whatever we do with whatever we have to work with, productivity is really more important than ever. And so, you could see that our company is indexed to two things, both of which are more important than ever, which is power efficiency, cost efficiency and then, of course, productivity. And these things are more important than ever. And my expectation is that we're seeing all the strong demand and surging demand for AI and for niche reasons.\\n\\nOperator\\n\\nYour next question comes from the line of C.J. Muse with Evercore. Your line is now open.\\n\\nC.J. Muse -- Evercore ISI -- Analyst\\n\\nYeah, Good afternoon and thank you for taking the question. You started to bundle on NVIDIA enterprise now with the H-100. I'm curious if you can talk about how we should think about timing around software monetization? And how we should kind of see this flow through the model, particularly with the focus on the AI enterprise and Omnivere side of things?\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nYes. Thanks, CJ. We're making excellent progress in NVIDIA AI enterprise. In fact, you saw probably that we made several announcements this quarter associated with clouds.\\n\\nYou know that NVIDIA has a rich ecosystem. And over the years, our rich ecosystem and our software stack has been integrated into developers and start-ups of all kinds, but more so -- more than ever, we're at the tipping point of clouds, and that's fantastic. Because if we could get NVIDIA's architecture and our full stack into every single cloud, we could reach more customers more quickly. And this quarter, we announced several initiatives, one has several partnerships and collaborations, one that we announced today, which has to do with Microsoft and our partnership there.\\n\\nIt has everything to do with scaling up AI because we have so many start-ups clamoring for large installations of our GPU so that they could do large language model training and building their start-ups and scale out of AI to enterprise and all of the world's Internet service providers. Every company we're talking to would like to have the agility and the scale, flexibility of clouds. And so, over the last year or so, we've been working on moving all of our software stacks to the cloud are of our platform and software stacks to the cloud. And so, today, we announced that Microsoft and ourselves are going to standardize on the NVIDIA stack, for a very large part of the work that we're doing together so that we could take a full stack out to the world's enterprise.\\n\\nThat's all software included. We, a month ago, announced the same similar type of partnership with Oracle. You also saw that rescale a leader in high-performance computing cloud has integrated NVIDIA AI into their stack. [Inaudible] has been integrated into GCP.\\n\\nAnd we announced recently Nemo, large language model and bionemo large language model to put NVIDIA software in the cloud. And we also announced Omniverse is now available in the cloud. The goal of all of this is to move the NVIDIA platform full stack off boarding the cloud so that we can engage customers much, much more quickly and customers could engage our software if they would like to use it in the cloud, it's per GPU instance hour if they would like to utilize our software on-prem, they could do it through software license. And so, license and subscription.\\n\\nAnd so, in both cases, we now have software available practically everywhere you would like to engage it. The partners that we work with are super excited about it because MBDA's rich ecosystem is global, and this could bring both new consumption into their cloud for both them and ourselves, but also connect all of these new opportunities to the other APIs and other services that they offer. And so, our software stack is making really great progress.\\n\\nOperator\\n\\nYour next question comes from the line of Chris Caso with Credit Suisse. Your line is now open.\\n\\nChris Caso -- Credit Suisse -- Analyst\\n\\nYes. Thank you. Good evening. I wonder if you could give some more color about the inventory charges you took in the quarter and then internal inventory in general.\\n\\nIn the documentation, you talked about that being a portion of inventory on hand plus some purchase obligations. And you also spoke in your prepared remarks that some of this was due to China data centers. So if you can clarify what was in those charges. And then, in general, for your internal inventory.\\n\\nDoes that still need to be worked down? And what are the implications if that needs to be worked down over the next couple of quarters?\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nThanks for the question, Chris. So as we highlighted in our prepared remarks, we booked an entry of $702 million for inventory reserves within the quarter. Most of that, primarily, all of it is related to our data center business, just due to the change in expected demand looking forward for China. So when we look at the data center products, a good portion of this was also the A100, which we wrote down.\\n\\nNow, looking at our inventory that we have on hand and the inventory that has increased, a lot of that is just due to our upcoming architectures coming to market. our ADA architecture, our hopper architecture and even more in terms of our networking business. We have been building for those architectures to come to market and as such to say. We are always looking at our inventory levels at the end of each quarter for our expected demand going forward.\\n\\nBut I think we've done a solid job that we used in this quarter just based on that expectation going forward.\\n\\nOperator\\n\\nYour next question comes from the line of Timothy Arcuri with UBS. Your line is now open.\\n\\nTimothy Arcuri -- UBS -- Analyst\\n\\nThanks a lot. Colette, can you -- I have a two-part question. First, is there any effect of stockpiling in the data center guidance? I ask because you now have the A800 that is sort of a modified version of the A100 with the lower data transfer rate. So one could imagine that customers might be stocking that while they can still get it.\\n\\nAnd I guess the second part of that is related to the inventory charge, can you just go into that a little bit more? Because last quarter, it made sense that you took a charge because revenue was less than you thought, but revenue came in pretty much in line. And it sounded like China was a net neutral. So is the charge related to just working A100 inventory down faster? Is that what the charges related to?\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nSure. So let me talk about the first statement that you indicated. Most of our data center business that we see is we're working with customers specifically on their needs to build out accelerated computing and AI. It's just not a business in terms of where units are being held for that.\\n\\nThey're usually four very, very specific products and projects that we see. So I'm going to answer no. Nothing that we can see. Your second question regarding the inventory provisions.\\n\\nAt the end of last quarter, we were beginning to see softness in China. We've always been looking at our needs long term. It's not a statement about the current quarter in inventory, as you can see. It usually takes two or three quarters for us to build product for the future demand.\\n\\nSo that's always a case of the inventory that we are ordering. So now looking at what we've seen in terms of continued lockdowns, continued economy challenges in China it was time for us to take a hard look of what do we think we'll need for data center going forward and not leg for write-downs.\\n\\nOperator\\n\\nYour next question comes from the line of Stacy Rasgon with Bernstein. Your line is now open.\\n\\nStacy Rasgon -- AllianceBernstein -- Analyst\\n\\nHi, guys. Thanks for taking my question. Colette, I had a question on the commentary you gave on the sequentials. It kind of sounded like data center maybe had some China softness issues.\\n\\nYou said gaming resumed sequential growth. But then you said sequential growth for the company driven by auto gaming and data center. How can all three of those grow sequentially if the overall guidance is kind of flattish? Are they all just like growing just a little bit? Or is one of them actually down? Like how do we think about the segments into Q4 given that commentary?\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nYes. So your question is regarding the sequentials from Q3 to our guidance that we provided for Q4. As we are seeing the numbers in terms of our guidance, you're correct, is only growing about $100 million. And we've indicated that three of those platforms will likely grow just a little bit.\\n\\nBut our pro visualization business we think is going to be flattish and likely not growing as we're still working on correcting the channel inventory levels. to get to the right amount. It's very difficult to say which will have that increase. But again, we are planning for all three of those different market platforms to grow just a little bit.\\n\\nOperator\\n\\nYour next question comes from the line of Mark Lipacis with Jefferies. Your line is now open.\\n\\nMark Lipacis -- Jefferies -- Analyst\\n\\nHi. Thanks for taking my question. Jen-Hsun, I think for you, you've articulated a vision for the data center we're a solution with an integrated solution set of a CPU, GPU and DPU is deployed for all workloads or most workloads, I think. Could you just give us a sense of or talk about where is this vision in the penetration cycle? And maybe talk about Grace Grace's importance for realizing that vision, what will Grace deliver versus an off-the-shelf x86 where -- do you have a sense of where Grace will get embraced first or the fastest within that vision?\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nGrace's data moving capability is off the charts. Grace also is memory coherent to our GPU, which allows our GPU to expand its effective GPU memory, fast GPU memory by a factor of 10. That's not possible without special capabilities that are designed between hopper and Grace and the architecture of Grace. And so, it was designed.\\n\\nGrace is designed for very large data processing at very high speeds. Those applications are related to, for example, data processing is related for recommender systems, which operates on petabytes of live data at a time. It's all hot. It all needs to be fast, so that you can make a recommendation within milliseconds to hundreds of millions of people using our service.\\n\\nIt is also quite effective at AI training, machine learning. And so, those kind of applications are really terrific. We -- Grace, I think I've said before that we will have production samples in Q1, and we're still on track to do that.\\n\\nOperator\\n\\nYour next question comes from the line of Harlan Sur with J.P. Morgan. Your line is now open.\\n\\nHarlan Sur -- JPMorgan Chase and Company -- Analyst\\n\\nGood afternoon and thanks for taking my question. Your data center networking business, I believe, is driving about $800 million per quarter in sales, very, very strong growth over the past few years. Near term, as you guys pointed out, and the team is driving strong Nick and blue food attached to your own compute solutions like DGX and more partner announcements like VMware, but we also know that networking has pretty large exposure to general purpose cloud and hyperscale compute spending trends. So what's the visibility and growth outlook for the networking business over the next few quarters?\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nYes. If I could take that. First, thanks for your question. Our networking, as you know, is heavily indexed to high-performance computing.\\n\\nWe're not -- we don't serve the vast majority of commodity networking. All of our network solutions are very high end, and they're designed for data centers that move a lot of data. Now, if you have a hyperscale data center these days, and you are deploying a large number of AI applications. It is very likely that the network bandwidth that you provision has a substantial implication on the overall throughput of your data center.\\n\\nSo the small incremental investment they make in high-performance networking translates to billions of dollars of savings slightly in provisioning the service or billions of dollars more throughput, which increases their economics. And so, these days, with disaggregated and I application, AI provisioning and data centers, high-performance networking is really quite fantastic and it pays for itself right away. But that's where we are focused in high-performance networking and provisioning AI services in -- well, the AI applications that we focus on. You might have noticed that NVIDIA and Microsoft are building one of the largest AI infrastructures in the world.\\n\\nAnd it is completely powered by NVIDIA's InfiniBand 400 gigabits per second network. And the reason for that is because that network pays for itself instantaneously. The investment that you're going to put into the infrastructure is so significant that if you were to be dragged by slow networks, obviously, the efficiency of the overall infrastructure is not as high. And so, in the places where we focus networking is really quite important.\\n\\nIt goes all the way back to when we first announced the acquisition of Mellanox. I think at the time, they were doing about a few hundred million dollars a quarter, about $400 million a quarter. And now we're doing what they used to do in the old days, in a year, practically coming up in a quarter. And so, that kind of tells you about the growth of high-performance networking.\\n\\nIt is an indexed to overall enterprise and data center spend but it is highly indexed to AI adoption.\\n\\nOperator\\n\\nYour next question comes from the line of Aaron Rakers with Wells Fargo. Your line is now open.\\n\\nAaron Rakers -- Wells Fargo Securities -- Analyst\\n\\nThanks for taking the question. I want to expand on the networking question a little bit further. When we look at the Microsoft announcement today, we think about what Meda is doing on the AI footprint that they're deploying. Jen-Hsun, can you help us understand like where your InfiniBand networking sits relative to like traditional data center switching? And maybe kind of build on that, how you're positioning spectrum for in the market, does that compete against a broader set of opportunities in the Ethernet world for AI fabric networking?\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nYes. Thanks, Erin. The math is like this. If you're going to spend $20 billion on an infrastructure and the efficiency of that overall data center is improved by 10%.\\n\\nThe numbers are huge. And when we do these large language models and recommender systems, the processing is done across the entire data center. And so, we distribute the workload across multiple GPUs, multiple nodes and it runs for a very long time. And so, the importance of the network can be overemphasized.\\n\\nAnd so, the difference of 10% in overall improvement in efficiency, which is very to achieve. The difference between NVIDIA's InfiniBand, the entire software stack with what we call Magnum IO, which allows us to do computing in the network itself. A lot of software is running in the network itself, not just moving data around. We call it in-network computing because a ton of software is done at the edge at the -- within the network itself.\\n\\nWe achieved significant differences in overall efficiency. And so, if you're spending billions of dollars on the infrastructure, or even hundreds of millions of dollars of interest on the infrastructure. The difference is really quite profound.\\n\\nOperator\\n\\nYour next question comes from the line of Ambrish Srivastava with BMO. Your line is now open.\\n\\nAmbrish Srivastava -- BMO Capital Markets -- Analyst\\n\\nHi. Thank you very much. I actually had a couple of clarifications. Colette, in the data center side, is it a fair assumption that compute was down Q-over-Q in the reported quarter because the quarter before, Mellanox or the networking business was up as it was called out.\\n\\nAnd again, you said it grew quarter over quarter. So is that a fair assumption? And then, I had a clarification on the USG band. Initially, it was supposed to be a $400 million, really going to what the government was trying to firewall. Is the A800 -- I'm just trying to make sure I understand it.\\n\\nIsn't that against the spirit of what the government is trying to do, i.e., firewall, high-performance compute? Or is A800 going to a different set of customers?\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nThank you for the question. So looking at our compute for the quarter is about flattish. Yes, we're seeing also growth growth in terms of our networking, but you should look at our Q3 compute is about flatters with last quarter.\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nAmbrish, A800 hardware, the hardware of ensures that it always meets U.S. government's clear test for export control. And it cannot be customer reprogrammed or application reprogrammed to exceed it. It is hardware limited.\\n\\nIt is in the hardware that determines 800s capabilities. And so, it meets the clear test in letter and in spirit. We raised the concern about the $400 million of A100s because we were uncertain about whether we could execute. The introduction of A800 to our customers and through our supply chain in time.\\n\\nThe company did remarkable feeds to swarm this situation and make sure that our business was not affected and our customers were not affected. But A800 hardware surely ensures that it always meets U.S. government's clear tests for export control.\\n\\nOperator\\n\\nYour next question comes from the line of William Stein with Truist Securities. Your line is now open.\\n\\nWilliam Stein -- Truist Securities -- Analyst\\n\\nThank you. I'm hoping you can discuss the pace of 100 growth as we progress over the next year. We've gotten a lot of questions as to whether the ramp in this product should look like a sort of traditional product cycle where there's quite a bit of pent-up demand for this significant improved performance product and that there's supply available as well. So does this rollout sort of look relatively typical from that perspective? Or should we expect a more perhaps delayed start of the growth trajectory where we see maybe substantially more growth in, let's say, second half of '23.\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nH-100 ramp is different than the A100 ramp in several ways. The first is that the TCO, the cost benefits, the operational cost benefits because of the energy savings because every data center is now Power Limited. And because of this incredible transformer engine that's designed for the latest AI models. The performance over Ampere is so significant that I -- and because of the pent-up demand for hopper because of these new models that are that I spoke about earlier, deep recommender systems and large language models and generative AI models.\\n\\nCustomers are clamoring to ramp hopper as quickly as possible, and we are trying to do the same. We are all hands on deck to help the cloud service providers stand up the supercomputers. Remember, I is the only company in the world that produces and ships semi-custom supercomputers in high volume. It's a miracle to ship one supercomputer every three years.\\n\\nit's unheard of to ship supercomputers to every cloud service provider in a quarter. And so, we're working hand with every one of them, and every one of them are racing to stand up hoppers. We expect them to have hopper cloud services stood up in Q1. And so, we are expecting to ship some volume, we're expecting to ship production in Q4, and then we're expecting to ship large volumes in Q1.\\n\\nThat's a faster transition than MPIR. And so, it's because of the dynamics that I described.\\n\\nOperator\\n\\nYour next question comes from the line of Matt Ramsay with Cowen. Your line is now open.\\n\\nMatt Ramsay -- Cowen and Company -- Analyst\\n\\nYeah. Thank you very much. Good afternoon. I guess, Colette, I heard in your script that you had talked about maybe a new way of commenting on or reporting hyperscaler revenue in your data center business.\\n\\nAnd I wonder if you could maybe give us a little bit more detail about what you're thinking there and what sort of drove the decision? And I guess the derivative of that, Jen-Hsun, how -- that decision to talk about the data center business to hyperscalers differently. I mean, what does that mean for the business that is just a reflection of where demand is and you're going to break things out differently? Or is something changing about the mix of I guess, internal properties versus vertical industry demand within the hyperscale customer base.\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nYes, Matt, thanks for the question. Let me clarify a little bit in terms of what we believe we should be looking at when we go forward and discussing our data center business. Our data center business is becoming larger and larger and our customers are complex. And when we talk about hyperscale, we tend to talk about seven, eight different companies.\\n\\nBut the reality is there's a lot of very large companies that we could add to that discussion based on what they're purchasing. Additionally, looking at the cloud, looking at our cloud purchases and what our customers are building for the cloud is an important area to focus on because this is really where our enterprise is where our researchers, where our higher education is also purchasing. So we're trying to look for a better way to describe the color of what we're seeing in the cloud and also give you a better understanding of some of these large installments that we're seeing in the hyperscales.\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nYes. Let me double click on what Colette just said, which is absolutely right. There are two major dynamics that's happening. First, the adoption of NVIDIA in Internet service companies around the world, the number and the scale by which they're doing it has grown a lot.\\n\\nInternet service companies. And these are Internet service companies that offer services, but they're not public cloud computing companies. The second factor has to do with cloud computing. We are now at the tipping point of cloud computing.\\n\\nAlmost every enterprise in the world has both a cloud-first and a multi-cloud strategy. It is exactly the reason why all of the announcements that we made this year -- this quarter, this last quarter since GTC about all the new platforms that are now available in the cloud. a CSP, a hyperscaler is both -- are two things to us, therefore, a hyperscaler can be a sell to customer. They are also a cell with partner on the public cloud side of their business.\\n\\nBecause of the richness of NVIDIA's ecosystem because we have so many Internet service customers and enterprise customers using NVIDIA's full stack. The public cloud side of their business really enjoys and values the partnership with us and the cell with relationship they have with us. And it's pretty clear now that for all of the hyperscalers, the public cloud side of their business will likely would very likely be the vast majority of their overall consumption. And so, because the world CSPs, the world's public clouds is only at the early innings of their enterprise to lifting enterprise to the cloud world it's very, very clear that the public cloud side of the business is going to be very large.\\n\\nAnd so, increasingly, our relationship with CSPs, our relationship with hyperscalers will -- will include, of course, continuing to sell to them for internal consumption but very importantly, sell with for the public cloud side.\\n\\nOperator\\n\\nYour next question comes from the line of Joseph Moore with Morgan Stanley. Your line is now open.\\n\\nJoseph Moore -- Morgan Stanley -- Analyst\\n\\nGreat. Thank you. I wonder if you could talk to looking backward at the crypto impact. Obviously, that's gone from your numbers now, but do you see any potential for liquidation of GPUs that are in the mining network, any impact going forward? And do you foresee blockchain being an important part of your business at some point down the road?\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nWe don't expect to see blockchain being an important part of our business down the road. There is always a resell market. If you look at any of the major resell sites, eBay, for example, there are secondhand graphics cards for sale all the time. And the reason for that is because a 3090 that somebody bought today, is upgraded to a 4090 or 3090 by a couple of years ago, it was up are until 4090 today.\\n\\nThat 3090 could be sold to somebody and enjoyed it sold at the right price. And so, the volume of -- the availability of secondhand and used graphics cards has always been there. And the inventory is never zero. and when the inventory is larger than usual, like all supply demand, it would likely drift lower price and affect the lower ends of our market.\\n\\nBut my sense is that where we're going right now with ADA is targeting very clearly in the upper range, the top half of our market. And and early signs are, and I'm sure you're also seeing that the ADA launch was a home run. That we shipped a large volume of 4090s because as you know, we were prepared for it. And yet within minutes, they were sold out around the world.\\n\\nAnd so, the reception of 4090 and the reception of 4080 today has been off the charts. And that says something about the strength and the health and the vibrancy of the gaming market. So we're super enthusiastic about the ADA launch. We have many more ad products to come.\\n\\nOperator\\n\\nYour last question today comes from the line of Toshiya Hari with Goldman Sachs. Your line is now open.\\n\\nToshiya Hari -- Goldman Sachs -- Analyst\\n\\nGreat. Thank you so much for squeezing me in. I had two quick ones for Colette. On supply, I think there was some mixed messaging in your remarks.\\n\\nI think you talked about supply being a headwind at one point. And then, when you were speaking to the networking business, I think you talked about supply easing. So I was hoping you can kind of speak to supply if you're caught up to demand at this point. And then, secondly, just on stock-based compensation, pretty mundane topic I realize, but it is -- I think in the quarter, it was about $700 million.\\n\\nIt's becoming a bigger piece of your opex. So curious how we should be modeling that going forward.\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\nSure. When we look at our supply constraints that we have had in the past, each and every quarter, this is getting better Networking was one of our issues probably a year ago, and it has taken us probably to this quarter. and next quarter to really see our supply improved so that we can support the pipeline that we have for our customers that are -- now that's our supply. We've also made a discussion regarding our customers, supply constraints, issues when setting up a data center, even getting data center capacity has been very difficult.\\n\\nAnd therefore, that challenges them in their purchasing decisions as they're still looking for certain parts of that supply chain to come through. So that hopefully clarifies what we were talking about regarding two areas of supply. In our stock-based compensation, what we'll see, it's very difficult to predict what our stock-based compensation would be when it arrives. We have provided to our incoming employees but also once a year to our employees, and it's a single date in terms of when that is priced.\\n\\nSo it's difficult to determine, but stock-based compensation is an important part of our employees' compensation and will continue to be. So we look at it from an overall compensation perspective. So up until now and when we do the focal, we'll see about the same size with a few additions for the reduced level of employee hiring that we have right now.\\n\\nOperator\\n\\nThank you. I will now turn the call back over to Jen-Hsun Huang for closing remarks.\\n\\nJen-Hsun Huang -- President and Chief Executive Officer\\n\\nThanks, everyone. We are quickly adapting to the macro environment. Correcting inventory levels, offering alternative products to data center customers in China and keeping our opex flat for the next few quarters. Our new platforms are off to a great start and formed the foundation for our resumed growth.\\n\\nMRTX is reinventing 3D graphics with ray tracing and AI. The launch of [Inaudible] is phenomenal. Gamers waited in long lines around the world, 4090 stocks sold out quickly. Hopper, with its revolutionary transformer engine is just in time to meet the surging demand for recommender systems, large language models and generative AI.\\n\\nNVIDIA networking is synonymous with the highest data center throughput and enjoying record results. Oren is the world's first computing platform designed for AI-powered autonomous vehicles and robotics and putting automotive on the road to be our next multibillion-dollar platform. These computing platforms run NVIDIA AI and NVIDIA Omniverse, software libraries and engines that help the companies build and deploy AI to products and services. we this pioneering work and accelerated computing is more vital than ever.\\n\\nLimited by business, general purpose commuting has slowed to a crawl just as AI demands more computing. Scaling through general purchase computing alone is no longer viable, both from a cost or power standpoint. Accelerated computing is the path forward. We look forward to updating you on our progress next quarter.\\n\\ntl;dr:\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"{transcript}\\n\\ntl;dr:\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe034a03",
   "metadata": {},
   "source": [
    "Now that we have a prompt, let's call OpenAI using this prompt:\n",
    "\n",
    "> Indented block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b763f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    #engine=\"text-davinci-003\",\n",
    "    engine=\"text-davinci-003\n",
    "    prompt=prompt,\n",
    "    temperature=0.3, # The temperature controls the randomness of the response, represented as a range from 0 to 1. A lower value of temperature means the API will respond with the first thing that the model sees; a higher value means the model evaluates possible responses that could fit into the context before spitting out the result.\n",
    "    max_tokens=140,\n",
    "    top_p=1, # Top P controls how many random results the model should consider for completion, as suggested by the temperature dial, thus determining the scope of randomness. Top P’s range is from 0 to 1. A lower value limits creativity, while a higher value expands its horizons.\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a8ef8e",
   "metadata": {},
   "source": [
    "##Chunking up our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b2d2df6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Operator\\n\\nGood',\n",
       " 'afternoon.',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Emma,',\n",
       " 'and',\n",
       " 'I',\n",
       " 'will',\n",
       " 'be',\n",
       " 'your',\n",
       " 'conference',\n",
       " 'operator',\n",
       " 'today.',\n",
       " 'At',\n",
       " 'this',\n",
       " 'time,',\n",
       " 'I',\n",
       " 'would',\n",
       " 'like']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = transcript.split(\" \")\n",
    "\n",
    "# show the first 20 words\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d2e061c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7912"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd669f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Operator\\n\\nGood', 'afternoon.', 'My', ..., 'total', 'number',\n",
       "        'of'], dtype='<U28'),\n",
       " array(['RTX', 'games', 'and', ..., 'path', 'forward', 'as'], dtype='<U28'),\n",
       " array(['general', 'purpose', 'computing', ..., 'that', 'while', 'they'],\n",
       "       dtype='<U28'),\n",
       " array(['can', 'still', 'get', ..., 'taking', 'the', 'question.'],\n",
       "       dtype='<U28'),\n",
       " array(['I', 'want', 'to', ..., 'Huang', '--', 'President'], dtype='<U28'),\n",
       " array(['and', 'Chief', 'Executive', ..., 'progress', 'next', 'quarter.'],\n",
       "       dtype='<U28')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "chunks = np.array_split(words, 6)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "589d434e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Operator\\n\\nGood afternoon. My name is Emma, and I will be your conference operator today. At this time, I would like to welcome everyone to the NVIDIA's third quarter earnings call. [Operator instructions] Simona Jankowski, you may begin your conference.\\n\\nSimona Jankowski -- Vice President, Investor Relations\\n\\nThank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2023. With me today from NVIDIA are Jen-Hsun Huang, president and chief executive officer; and Colette Kress, executive vice president and chief financial officer. I'd like to remind you that our call is being webcast live on NVIDIA's investor relations website.\\n\\n\\nThe webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter and fiscal 2023. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations.\\n\\nThese are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release our most recent Forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, November 16, 2022, and based on information currently available to us. Except as required by law, we assume no obligation to update any such statements.\\n\\nDuring this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.\\n\\nColette Kress -- Executive Vice President and Chief Financial Officer\\n\\n\\nThanks, Simona. Q3 revenue was $5.93 billion, down 12% sequentially and down 17% year on year. We delivered record data center and automotive revenue. while our gaming and pro visualization platforms declined as we work through channel inventory corrections and challenging external conditions.\\n\\nStarting with data center. Revenue of $3.83 billion was up 1% sequentially and 31% year-on-year. This reflects very solid performance in the face of macroeconomic challenges new export controls and lingering supply chain disruptions. Year-on-year growth was driven primarily by leading U.S.\\n\\ncloud providers and a broadening set of consumer Internet companies for workloads such as large language models, recommendation systems and generative AI. As the number and scale of public cloud computing and Internet service companies deploying NVIDIA AI grows our traditional hyperscale definition will need to be expanded to convey the different end market use cases. We will align our data center customer commentary going forward accordingly. Other vertical industries, such as automotive and energy, also contributed to growth with key workloads relating to autonomous driving, high-performance computing, simulations and analytics.\\n\\nDuring the quarter, the U.S. government announced new restrictions impacting exports of our A100 and H-100 based products to China, and any product destined for certain systems or entities in China. These restrictions impacted third quarter revenue, largely offset by sales of alternative products into China. That said, demand in China more broadly remains soft, and we expect that to continue in the current quarter.\\n\\n\\nWe started shipping our flagship 100 data center GPU based on the new hopper architecture in Q3. A100-based systems are available starting this month from leading server makers including Dell, Hewlett Packard Enterprise, Lenovo and SuperMicro. Early next year, the first H-100 based cloud instances will be available on Amazon Web Services, Google Cloud, Microsoft Azure and Oracle Cloud Infrastructure. A100 delivered the highest performance and workload versatility for both AI training and inference in the latest MLPerf industry benchmarks.\\n\\nH-100 also delivers incredible value compared to the previous generation for equivalent AI performance it offers three x lower total cost of ownership while using five x fewer server nodes and 3.5 x less energy. Earlier today, we announced a multiyear collaboration with Microsoft to build an advanced cloud-based AI supercomputer to help enterprises train, deploy and scale AI including large state-of-the-art models. MacBook Azure will incorporate our complete AI stack, adding tens and thousands of A100 and A100 GPUs. Quantum 2 400 gigabit per second InfiniBand networking and the NVIDIA AI enterprise software suite to its platform.\\n\\nOracle and NVIDIA are also working together to offer AI training and inference at scale to thousands of enterprises. This includes bringing to Oracle Cloud infrastructure, the full NVIDIA accelerated computing stack and adding tens of thousands of NVIDIA GPUs, including the A100 and H-100. Cloud-based high-performance in the company, new scale is adopting NVIDIA AI enterprise and other software to address the industrial scientific communities, rising demand for AI in the cloud. NVIDIA AI will bring new capability to rescale high-performance computing as a service offerings, which include simulation and engineering software used across industries.\\n\\nNetworking posted strong growth driven by hyperscale customers and easing supply constraints. -- our new Quantum 240 gigabit per second InfiniBand and Spectrum Ethernet networking platforms are building momentum. We achieved an important milestone this quarter with VMware. And whose leading server virtualization platform, vSphere, has been rearchitected over the last two years to run on DPUs and now supports our BlueField DPUs.\\n\\nOur joint enterprise AI platform is available first on Dell PowerEdge servers. The BlueField DPU design win pipeline is growing and the number of infrastructure softer partners is expanding, including Arista, Check Point, Juniper, [Inaudible] Networks and Red Hot. The latest top 500 list of supercomputers released this week at Supercomputing '22 and has the highest ever number of NVIDIA-powered systems, including 72% of the total and 90% of new systems on the list. Moreover, NVIDIA powers 23 of the top 30 of the Green 500 list, demonstrating the energy efficiency of accelerated computing.\\n\\nThe No. 1 most energy-efficient system is the Flat Iron Institute Henry, which is the first top 500 system featuring our H-100 GPUs. At GTC, we announced the NVIDIA Omniverse Computing System, or OVS, reference designs featuring the new L4 GPU based on the ADA Lovelace architecture. These systems are designed to build and operate 3D virtual world using NVIDIA Omniverse enterprise.\\n\\nNVIDIA OBX systems will be available from Inspur, Lenovo and Super Micro by early 2023. We Lockheed Martin and Jaguar Land Rover will be among the first customers to receive OVS systems. We are further expanding our AI software and services offerings with NVIDIA and Bio Nemo large language model services, which are both entering early access this month. These enable developers to easily adopt large language models and deploy customized AI applications for content generation, tech summarization, chatbox, co-development, protein structure and biomolecular property predictions.\\n\\nMoving to gaming. Revenue of $1.57 billion was down 23% sequentially and down 51% from a year ago, reflecting lower sell-in to partners to help align channel inventory levels with current demand expectations. We believe Channel inventories are on track to approach normal levels as we exit Q4. Sell-through for our gaming products was relatively solid in the Americas and EMEA and but softer in Asia Pac as macroeconomic conditions and covered lockdowns in China continued to weigh on consumer demand.\\n\\nOur new Ada Lovelace GPU architecture had an exceptional launch. The first ADA GPU, the GeForce RTX 4090 became available in mid-October and a tremendous amount and positive feedback from the gaming community. We sold out quickly in many locations and are working hard to keep up with demand. The next member of the ATA family, RTX 4080 is available today.\\n\\nThe RTX 40 Series GPUs features DLSS 3, the neuro rendering technology that uses AI to generate entire frames for faster game play. Our third-generation RTX technology has raised the bar for computer graphics and help supercharge gaming. For example, the 15-year old classic game portal, now reimagined with full ray tracing and DLSS 3 has made it on Steam's top 100 most wish-listed gains. The total number of\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ' '.join(list(chunks[0]))\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"{sentences}\\n\\ntl;dr:\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\", \n",
    "    prompt=prompt,\n",
    "    temperature=0.3, # The temperature controls the randomness of the response, represented as a range from 0 to 1. A lower value of temperature means the API will respond with the first thing that the model sees; a higher value means the model evaluates possible responses that could fit into the context before spitting out the result.\n",
    "    max_tokens=140,\n",
    "    top_p=1, # Top P controls how many random results the model should consider for completion, as suggested by the temperature dial, thus determining the scope of randomness. Top P’s range is from 0 to 1. A lower value limits creativity, while a higher value expands its horizons.\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=1\n",
    ")\n",
    "\n",
    "response_text = response[\"choices\"][0][\"text\"]\n",
    "response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_responses = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    \n",
    "    sentences = ' '.join(list(chunk))\n",
    "\n",
    "    prompt = f\"{sentences}\\n\\ntl;dr:\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\", \n",
    "        prompt=prompt,\n",
    "        temperature=0.3, # The temperature controls the randomness of the response, represented as a range from 0 to 1. A lower value of temperature means the API will respond with the first thing that the model sees; a higher value means the model evaluates possible responses that could fit into the context before spitting out the result.\n",
    "        max_tokens=150,\n",
    "        top_p=1, # Top P controls how many random results the model should consider for completion, as suggested by the temperature dial, thus determining the scope of randomness. Top P’s range is from 0 to 1. A lower value limits creativity, while a higher value expands its horizons.\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=1\n",
    "    )\n",
    "\n",
    "    response_text = response[\"choices\"][0][\"text\"]\n",
    "    summary_responses.append(response_text)\n",
    "\n",
    "full_summary = \"\".join(summary_responses)\n",
    "\n",
    "print(\"full summary\")\n",
    "print(full_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
